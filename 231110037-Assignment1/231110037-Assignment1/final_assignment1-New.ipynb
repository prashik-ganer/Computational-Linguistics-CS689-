{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hi_100.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read and print the content\n",
    "#     print(file.read())\n",
    "    content=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matra = ['अ','ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः','ँ',' ृ',' ॄ']\n",
    "# vowels2 = ['आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ']\n",
    "# vowels = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ']\n",
    "vowels = ['अ','आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः','अँ','ऋ', 'ॠ']\n",
    "vowelsCheck=['अ','आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः']\n",
    "consonants = ['क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह']\n",
    "extraSounds=['ं', 'ः','ँ']\n",
    "rev_dict={'अ':'','आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ','अं':'ं', 'अः':'ः','अँ':'ँ','ऋ':' ृ','ॠ':' ॄ'}\n",
    "# Create a dictionary mapping\n",
    "mapping_dict = dict(zip(matra, vowels))\n",
    "# mapping_dict['अ']='अ'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_haOqqlkwJda",
    "outputId": "b1975d29-dea7-44df-a1b7-27b6ffe96137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म', 'ो', 'द', 'ी'], ['स', 'र', 'क', 'ा', 'र'], ['क', 'े'], ['प', 'ह', 'ल', 'े'], ['क', 'ा', 'र', '्', 'य', 'क', 'ा', 'ल'], ['म', 'े', 'ं'], ['भ', 'ी'], ['त', 'ी', 'न'], ['त', 'ल', 'ा', 'क'], ['क', 'ो'], ['ल', 'े', 'क', 'र'], ['ब', 'ि', 'ल'], ['ल', 'ा', 'य', 'ा'], ['ग', 'य', 'ा'], ['थ', 'ा', ','], ['ह', 'ा', 'ल', 'ा', 'ं', 'क', 'ि'], ['त', 'ब'], ['य', 'ह'], ['र', 'ा', 'ज', '्', 'य', 'स', 'भ', 'ा'], ['म', 'े', 'ं']]\n",
      "[['म्', 'ओ', 'द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ', 'क्', 'आ', 'ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ', 'ल्', 'आ', 'क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'इ', 'ल्', 'अ'], ['ल्', 'आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ', 'स्', 'अ', 'भ्', 'आ'], ['म्', 'ए', 'अं']]\n"
     ]
    }
   ],
   "source": [
    "matra = ['अ','ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः','ँ',' ृ',' ॄ']\n",
    "# vowels2 = ['आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ']\n",
    "# vowels = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ']\n",
    "vowels = ['अ','आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः','अँ','ऋ', 'ॠ']\n",
    "vowelsCheck=['अ','आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः']\n",
    "consonants = ['क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह']\n",
    "extraSounds=['ं', 'ः','ँ']\n",
    "rev_dict={'अ':'','आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ','अं':'ं', 'अः':'ः','अँ':'ँ','ऋ':' ृ','ॠ':' ॄ'}\n",
    "\n",
    "\n",
    "# Create a dictionary mapping\n",
    "mapping_dict = dict(zip(matra, vowels))\n",
    "# mapping_dict['अ']='अ'\n",
    "\n",
    "\n",
    "txt =content\n",
    "\n",
    "lst = []\n",
    "temp=[]\n",
    "\n",
    "\n",
    "# Breaking string into different subparts\n",
    "for i in range(len(txt)+1):\n",
    "    # if(i not in consonants or i not in vowels or i not in matra):\n",
    "    #     continue\n",
    "    if(i==len(txt) or txt[i]==' '):\n",
    "        lst.append(temp)\n",
    "        temp=[]\n",
    "    else:\n",
    "        temp.append(txt[i])\n",
    "print(lst[:20])\n",
    "# [['ह', 'ि', 'न', '्', 'द', 'ी'], ['भ', 'ा', 'ष', 'ा'], ['क', 'ा'], ['अ', 'द', '्', 'व', 'ि', 'त', 'ी', 'य'], ['स', '्', 'थ', 'ा', 'न'], ['ह', 'ै', '।'], ['आ', 'ज'], ['ह', 'म'], ['आ', 'प', 'क', 'े'], ['ल', 'ि', 'ए'], ['ल', 'ा', 'य', 'े'], ['ह', 'ै'], ['ज', 'ि', 'ं', 'द', 'ग', 'ी'], ['प', 'र'], ['श', 'ा', 'य', 'र', 'ी'], ['द', 'ो'], ['ल', 'ा', 'इ', 'न'], ['ज', 'ि', 'न', 'क', 'ो'], ['आ', 'प'], ['अ', 'प', 'न', 'े'], ['द', 'ो', 'स', '्', 'त', 'ो', 'ं'], ['क', 'े'], ['स', 'ा', 'थ'], ['स', 'ा', 'ँ', 'झ', 'ा'], ['क', 'र'], ['स', 'क', 'त', 'े'], ['ह', 'ो', '।'], ['अ', 'प', 'न', 'े'], ['द', 'ो', 'स', '्', 'त', 'ो', 'ं'], ['क', 'े'], ['स', 'ा', 'थ'], ['स', 'ा', 'ँ', 'झ', 'ा'], ['क', 'र'], ['स', 'क', 'त', 'े'], ['ह', 'ो', '।'], ['स', '्', 'व', 'र', '्', 'ग'], ['आ', 'ं', 'ट', 'ी'], ['च', 'ा', 'ँ', 'द']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- FORMING UNICODE ----------------------------------\n",
    "separate_words=[]\n",
    "final_list=[]\n",
    "\n",
    "\n",
    "for word in range(len(lst)):\n",
    "\n",
    "    # for i in ranghe(len(lst[word])):\n",
    "    for i in range(len(lst[word])):\n",
    "        if(lst[word][i] in consonants and i+1<=len(lst[word])-1 and lst[word][i+1] in consonants):\n",
    "            separate_words.append(lst[word][i]+'्')\n",
    "            # lst[word][i]=lst[word][i]+'्'\n",
    "            # lst[word].insert(i+1, 'अ')\n",
    "            separate_words.append('अ')\n",
    "\n",
    "\n",
    "        # If the consonant is at the final index\n",
    "        elif(lst[word][i] in consonants and i==len(lst[word])-1):\n",
    "            separate_words.append(lst[word][i]+'्')\n",
    "            separate_words.append('अ')\n",
    "\n",
    "\n",
    "\n",
    "        elif(lst[word][i] in consonants and i+1<=len(lst[word])-1 and lst[word][i+1] not in consonants):\n",
    "            separate_words.append(lst[word][i]+'्')\n",
    "\n",
    "        elif(lst[word][i] in matra):\n",
    "            # lst[word][i]=mapping_dict[lst[word][i]]\n",
    "            separate_words.append(mapping_dict[lst[word][i]])\n",
    "\n",
    "        elif(lst[word][i] in vowels):\n",
    "            # lst[word][i]=mapping_dict[lst[word][i]]\n",
    "            separate_words.append(lst[word][i])\n",
    "\n",
    "\n",
    "    final_list.append(separate_words)\n",
    "    separate_words=[]\n",
    "\n",
    "# FORMING UNICODE\n",
    "print(final_list[:20])\n",
    "# [['ह्', 'इ', 'न्', 'द्', 'ई'], ['भ्', 'आ', 'ष्', 'आ'], ['क्', 'आ'], ['अ', 'द्', 'व्', 'इ', 'त्', 'ई', 'य्', 'अ'], ['स्', 'थ्', 'आ', 'न्', 'अ'], ['ह्', 'ऐ'], ['आ', 'ज्', 'अ'], ['ह्', 'अ', 'म्', 'अ'], ['आ', 'प्', 'अ', 'क्', 'ए'], ['ल्', 'इ', 'ए'], ['ल्', 'आ', 'य्', 'ए'], ['ह्', 'ऐ'], ['ज्', 'इ', 'अं', 'द्', 'अ', 'ग्', 'ई'], ['प्', 'अ', 'र्', 'अ'], ['श्', 'आ', 'य्', 'अ', 'र्', 'ई'], ['द्', 'ओ'], ['ल्', 'आ', 'इ', 'न्', 'अ'], ['ज्', 'इ', 'न्', 'अ', 'क्', 'ओ'], ['आ', 'प्', 'अ'], ['अ', 'प्', 'अ', 'न्', 'ए'], ['द्', 'ओ', 'स्', 'त्', 'ओ', 'अं'], ['क्', 'ए'], ['स्', 'आ', 'थ्', 'अ'], ['स्', 'आ', 'अँ', 'झ्', 'आ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'क्', 'अ', 'त्', 'ए'], ['ह्', 'ओ'], ['अ', 'प्', 'अ', 'न्', 'ए'], ['द्', 'ओ', 'स्', 'त्', 'ओ', 'अं'], ['क्', 'ए'], ['स्', 'आ', 'थ्', 'अ'], ['स्', 'आ', 'अँ', 'झ्', 'आ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'क्', 'अ', 'त्', 'ए'], ['ह्', 'ओ'], ['स्', 'व्', 'अ', 'र्', 'ग्', 'अ'], ['आ', 'अं', 'ट्', 'ई'], ['च्', 'आ', 'अँ', 'द्', 'अ']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************   FINAL OUTPUT  ***********************\n",
      "sortedCharacterCount_desc_var\n",
      "{'अ': 102, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "sortedSyllableCount_desc_var\n",
      "{'र': 13, 'स': 10, 'न': 10, 'क': 9, 'का': 6, 'के': 5, 'ल': 5, 'में': 5, 'या': 5, 'य': 5, 'म': 5, 'ह': 4, 'त': 4, 'ग': 4, 'हा': 4, 'ज': 4, 'प': 3, 'ले': 3, 'को': 3, 'कि': 3}\n",
      "bi_sortedCharacterCount_desc\n",
      "{'र्अ': 16, 'स्अ': 10, 'न्अ': 10, 'य्अ': 9, 'क्अ': 9, 'अर्': 8, 'अह्': 8, 'अन्': 8, 'य्आ': 7, 'अक्': 6, 'क्आ': 6, 'आर्': 6, 'एअं': 6, 'आय्': 6, 'अम्': 6, 'क्ए': 5, 'प्अ': 5, 'ल्अ': 5, 'म्ए': 5, 'त्अ': 5}\n",
      "bi_sortedSyllableCount_desc\n",
      "{'सर': 2, 'रका': 2, 'कार': 2, 'कर': 2, 'गया': 2, 'नहीं': 2, 'महा': 2, 'हाज': 2, 'जन': 2, 'इस': 2, 'और': 2, 'चेय': 2, 'यर': 2, 'रमै': 2, 'मैन': 2, 'मोदी': 1, 'पह': 1, 'हले': 1, 'कार्य': 1, 'र्यका': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"***********************   CHARACTER COUNTS ***********************\")\n",
    "# Takes input as unicode format list of list\n",
    "def sortedCharacterCount_desc(final_list):\n",
    "  characterCount={}\n",
    "\n",
    "  for word in range(len(final_list)):\n",
    "      for j in range(len(final_list[word])):\n",
    "          characterCount[final_list[word][j]]=characterCount.get(final_list[word][j],0)+1\n",
    "\n",
    "\n",
    "  # print(characterCount)\n",
    "  # sorted_dict = sorted(characterCount.items(), key=lambda x: x[1])\n",
    "  sortedCharacterCount_desc = dict(sorted(characterCount.items(), key=lambda item: item[1], reverse=True))\n",
    "  # top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc.items())[:20])\n",
    "\n",
    "  return sortedCharacterCount_desc\n",
    "  # return top_20_sortedCharacterCount_desc\n",
    "\n",
    "\n",
    "sortedCharacterCount_desc_var=sortedCharacterCount_desc(final_list)\n",
    "# print(sortedCharacterCount_desc_var[:20])\n",
    "\n",
    "\n",
    "\n",
    "# print(\"***********************   SYLLABLE GENERATOR ***********************\")\n",
    "# Takes input as unicode format list of list\n",
    "def syllable_generator(final_list):\n",
    "  tempList=[]\n",
    "  tempListFinal=[]\n",
    "  for sublist in range(len(final_list)):\n",
    "      # for i in range(len(sublist)):\n",
    "      i=0\n",
    "      # temp=[]\n",
    "      temp=\"\"\n",
    "      total=[]\n",
    "      # while(final_list[sublist][i] not in vowels):\n",
    "      #     temp.append(final_list[sublist][i])\n",
    "\n",
    "      for i in range(len(final_list[sublist])):\n",
    "          if(final_list[sublist][i] not in vowels or final_list[sublist][i] in extraSounds):\n",
    "              # temp.append(final_list[sublist][i])\n",
    "              temp+=final_list[sublist][i]\n",
    "              ''' FOR TESTING 1\n",
    "              print(\"1\")\n",
    "              print(temp)\n",
    "              '''\n",
    "          else:\n",
    "              # temp.append(final_list[sublist][i])\n",
    "              if(temp.endswith('्')):\n",
    "                temp=temp[:-1]\n",
    "              if(i==0):\n",
    "                temp+=final_list[sublist][i]\n",
    "              else:\n",
    "                temp+=rev_dict[final_list[sublist][i]]\n",
    "              # process(temp)\n",
    "              # Code for process(temp)\n",
    "              ''' FOR TESTING 2\n",
    "              print(\"2\")\n",
    "              size=len(temp)\n",
    "              print(temp)\n",
    "              '''\n",
    "              if(i<=len(final_list[sublist])-2 and final_list[sublist][i+1] in vowels):\n",
    "                # temp+=rev_dict[final_list[sublist][i]]\n",
    "                continue\n",
    "\n",
    "              tempList.append(temp)\n",
    "\n",
    "              temp=\"\"\n",
    "              # temp[size-2]=temp[size-2]+'अ'\n",
    "              # temp[size-2]=temp[size-2]+temp[size-1]\n",
    "              # temp.pop()\n",
    "              # print(temp)\n",
    "              total.append(temp)\n",
    "\n",
    "      if(tempList!=[]):\n",
    "        tempListFinal.append(tempList)\n",
    "      tempList=[]\n",
    "  return tempListFinal\n",
    "\n",
    "tempListFinal = syllable_generator(final_list)\n",
    "# print(tempListFinal)\n",
    "# [['हि', 'न्दी'], ['भा', 'षा'], ['का'], ['अ', 'द्वि', 'ती', 'य'], ['स्था', 'न'], ['है'], ['आ', 'ज'], ['ह', 'म'], ['आ', 'प', 'के'], ['लिे'], ['ला', 'ये'], ['है'], ['जिं', 'द', 'गी'], ['प', 'र'], ['शा', 'य', 'री'], ['दो'], ['लाि', 'न'], ['जि', 'न', 'को'], ['आ', 'प'], ['अ', 'प', 'ने'], ['दो', 'स्तों'], ['के'], ['सा', 'थ'], ['साँ', 'झा'], ['क', 'र'], ['स', 'क', 'ते'], ['हो'], ['अ', 'प', 'ने'], ['दो', 'स्तों'], ['के'], ['सा', 'थ'], ['साँ', 'झा'], ['क', 'र'], ['स', 'क', 'ते'], ['हो'], ['स्व', 'र्ग'], ['आं', 'टी'], ['चाँ', 'द']]\n",
    "\n",
    "\n",
    "# print(\"***********************   SYLLABLE COUNTS ***********************\")\n",
    "# Takes input as list of list of tempList\n",
    "def sortedSyllableCount_desc(tempListFinal):\n",
    "\n",
    "  syllableCount={}\n",
    "\n",
    "  for word in range(len(tempListFinal)):\n",
    "      for j in range(len(tempListFinal[word])):\n",
    "        # print(tempListFinal[word][j])\n",
    "        syllableCount[tempListFinal[word][j]]=syllableCount.get(tempListFinal[word][j],0)+1\n",
    "\n",
    "  # print(syllableCount)\n",
    "  sortedSyllableCount_desc = dict(sorted(syllableCount.items(), key=lambda item: item[1], reverse=True))\n",
    "  # top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc.items())[:20])\n",
    "\n",
    "  return sortedSyllableCount_desc\n",
    "  # return top_20_sortedSyllableCount_desc\n",
    "\n",
    "sortedSyllableCount_desc_var = sortedSyllableCount_desc(tempListFinal)\n",
    "# print(sortedSyllableCount_desc_var)\n",
    "\n",
    "\n",
    "# print(\"***********************   CHARACTER Bi-GRAM ***********************\")\n",
    "def bi_characterCount(final_list):\n",
    "  bi_CharacterCount={}\n",
    "  for i in range(len(final_list)):\n",
    "    for j in range(len(final_list[i])-1):\n",
    "      # final_list[i][j]\n",
    "      bi_Character=final_list[i][j]+final_list[i][j+1]\n",
    "      bi_CharacterCount[bi_Character]=bi_CharacterCount.get(bi_Character,0)+1\n",
    "\n",
    "  # print(bi_CharacterCount)\n",
    "  bi_sortedCharacterCount_desc = dict(sorted(bi_CharacterCount.items(), key=lambda item: item[1], reverse=True))\n",
    "  # top_20_bi_sortedCharacterCount_desc = dict(list(bi_sortedCharacterCount_desc.items())[:20])\n",
    "\n",
    "  return bi_sortedCharacterCount_desc\n",
    "  # return top_20_bi_sortedCharacterCount_desc\n",
    "\n",
    "# bi_characterCount(final_list)\n",
    "bi_sortedCharacterCount_desc=bi_characterCount(final_list)\n",
    "# print(bi_sortedCharacterCount_desc)\n",
    "# print(bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"***********************   SYLLABLE Bi-GRAM  ***********************\")\n",
    "def bi_SyllableCount(tempListFinal):\n",
    "  bi_SyllableCount={}\n",
    "  for i in range(len(tempListFinal)):\n",
    "    for j in range(len(tempListFinal[i])-1):\n",
    "      # tempListFinal[i][j]\n",
    "      bi_Syllable=tempListFinal[i][j]+tempListFinal[i][j+1]\n",
    "      bi_SyllableCount[bi_Syllable]=bi_SyllableCount.get(bi_Syllable,0)+1\n",
    "\n",
    "  # print(bi_SyllableCount)\n",
    "  bi_sortedSyllableCount_desc = dict(sorted(bi_SyllableCount.items(), key=lambda item: item[1], reverse=True))\n",
    "  # top_20_bi_sortedSyllableCount_desc = dict(list(bi_sortedSyllableCount_desc.items())[:20])\n",
    "\n",
    "  return bi_sortedSyllableCount_desc\n",
    "  # return top_20_bi_sortedSyllableCount_desc\n",
    "\n",
    "bi_sortedSyllableCount_desc=bi_SyllableCount(tempListFinal)\n",
    "# print(bi_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"***********************   FINAL OUTPUT  ***********************\")\n",
    "print(\"sortedCharacterCount_desc_var\")\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc_var.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc_var\")\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc_var.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_sortedCharacterCount_desc\")\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_sortedCharacterCount_desc.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "print(\"bi_sortedSyllableCount_desc\")\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_sortedSyllableCount_desc.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i05XZ_kgugh0",
    "outputId": "604e91d6-06b3-44d5-c599-ddf7b7714a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\prash\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "  pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIGRAM (Vocab size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O820YNUfMzdz",
    "outputId": "ce00c965-105f-4c6a-9da0-623220d131ad"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Load your text data\n",
    "text_data = \"hi_100.txt\"\n",
    "# Train the SentencePiece tokenizer\n",
    "spm.SentencePieceTrainer.train(input=text_data, model_prefix='unigram_model', model_type='unigram', vocab_size=1000)\n",
    "\n",
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('unigram_model.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "# sentence=content\n",
    "# tokens = sp.encode(sentence, out_type=str)\n",
    "\n",
    "# return tokens\n",
    "# print(tokens[:20])\n",
    "\n",
    "\n",
    "def unigram(content):\n",
    "    sentence=content\n",
    "    tokens = sp.encode(sentence, out_type=str)\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "lst = []\n",
    "temp=[]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁मोदी', '▁सरकार', '▁के', '▁पहले', '▁कार्य', 'का', 'ल', '▁में', '▁भी', '▁तीन', '▁त', 'ला', 'क', '▁को', '▁लेकर', '▁बि', 'ल', '▁ल', 'ाया', '▁गया', '▁था', ',', '▁हालांकि', '▁तब', '▁यह', '▁राज्य', 'स', 'भा', '▁में', '▁पास', '▁नहीं', '▁हो', '▁प', 'ाया', '▁था', '.', '▁भाजपा', '▁के', '▁द', 'ि', 'व', 'ंग', 'त', '▁ने', 'ता', '▁प्र', 'म', 'ो', 'द', '▁महा', 'जन', '▁की', '▁बे', 'टी', '▁प', 'ू', 'न', 'म', '▁महा', 'जन', '▁को', '▁सचिव', '▁बना', 'या', '▁गया', '▁है', '.', '▁ऐसी', '▁स्थित', 'ि', '▁में', '▁एक', '▁न्याय', 'पूर्ण', '▁सरकार', '▁सा', 'र', '्व', 'जन', 'िक', '▁वि', 'त्त', '▁का', '▁इस', '▁तरह', '▁इस्तेमाल', '▁कर', 'ती', '▁है', '▁कि', '▁सं', 'सा', 'ध', 'न', 'ों', '▁का', '▁आ', 'व', 'ंट', 'न', ',', '▁सभी', '▁के', '▁उप', 'भ', 'ो', 'ग', '▁वाले', '▁उत्पाद', 'ों', '▁की', '▁व', '्य', 'व', 'हा', 'र', '्य', 'ता', '▁और', '▁सम', 'ग्र', '▁व', 'ृ', 'ह', 'द', '-', 'आ', 'र्थ', 'िक', '▁प्र', 'बंध', 'न', \"▁'\", 'न', 'ि', 'ष', '्', 'प', 'क्ष', 'ता', '▁के', '▁रूप', '▁में', '▁न्याय', \"'\", '▁को', '▁बढ़', 'ा', 'ए', '।', '▁दिल', 'च', 'स्प', '▁है', '▁कि', '▁', 'डी', 'सी', 'ए', 'च', 'ए', 'ल', '▁के', '▁च', 'े', 'य', 'र', 'म', 'ै', 'न', '▁', 'टी', '▁वे', 'ंक', 'टर', 'म', 'न', '▁रे', 'ड्ड', 'ी', '▁और', '▁व', 'ाइ', 'स', '▁च', 'े', 'य', 'र', 'म', 'ै', 'न', '▁', 'टी', '▁वि', 'न', 'ाय', 'क', '▁र', 'वि', '▁इस', '▁बैठक', '▁में', '▁मौजूद', '▁नहीं', '▁थे', '।']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating unigram code from the given token list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['आ'], ['व्', 'ए'], ['द्', 'अ'], ['न्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'न्', 'ए'], ['क्', 'ई'], ['आ', 'ख्', 'इ', 'र्', 'अ'], ['ई'], ['त्', 'आ'], ['र्', 'ई'], ['ख्', 'अ'], [], [], ['ज्', 'अ', 'न्', 'अ', 'व्', 'अ', 'र्', 'ई'], [], [], [], ['ह्', 'ऐ'], [], ['इ', 'त्', 'अ', 'न्', 'अ']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def unigramCharacter(tokens):\n",
    "  # [['प्', 'र्', 'अ'], ['इ', 'य्', 'आ', 'अं'], ['श्', 'अ'], ['उ'], ['ह्', 'अ'], ['य्', 'अ'], ['उ'], ['अं', 'द्', 'अ'], ['ई'], ['च्', 'अ'], ['आ'], ['अँ'], ['द्', 'अ'], ['द्', 'इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'प्', 'अ'], ['क्', 'अ', 'र्', 'अ'], [], ['ण्', 'अ'], ['क्', 'अ'], ['अं', 'स्', 'अ'], ['ह्', 'इ'], ['न्', 'द्', 'अ'], ['ई'], ['म्', 'ए', 'अं'], ['आ'], ['व्', 'ए'], ['द्', 'अ'], ['न्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'न्', 'ए'], ['क्', 'ई'], ['आ', 'ख्', 'इ', 'र्', 'अ'], ['ई'], [], [], [], [], [], [], [], [], ['त्', 'आ'], ['र्', 'ई'], ['ख्', 'अ'], [], [], ['ज्', 'अ', 'न्', 'अ', 'व्', 'अ', 'र्', 'ई'], [], [], [], ['ह्', 'ऐ'], [], ['इ', 'त्', 'अ', 'न्', 'अ'], ['ई'], ['द्', 'उ'], ['आ'], ['क्', 'अ', 'र्', 'अ'], ['द्', 'ओ'], ['ह्', 'अ', 'म्', 'आ', 'र्', 'ए'], ['ल्', 'इ', 'ए'], ['क्', 'इ'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['ज्', 'अ'], ['इ', 'त्', 'अ'], ['न्', 'आ'], ['प्', 'अ'], ['य्', 'आ'], ['र्', 'अ'], ['द्', 'उ', 'न्', 'इ', 'य्', 'आ'], ['न्', 'ए'], ['आ', 'प्', 'अ', 'क्', 'ओ'], ['द्', 'इ', 'य्', 'आ'], ['ह्', 'ऐ'], [], ['ब्', 'अ'], ['स्', 'अ'], ['उ'], ['त्', 'अ'], ['न्', 'आ'], ['ह्', 'ई'], ['ह्', 'अ', 'म्', 'अ'], ['ए', 'अं'], ['भ्', 'ई'], ['म्', 'इ', 'ल्', 'अ'], ['ज्', 'आ', 'ए'], [], ['म्', 'ओ', 'द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'आ'], ['ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'इ'], ['ल्', 'अ'], ['ल्', 'अ'], ['आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], [], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ'], ['भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'अ'], ['आ', 'य्', 'आ'], ['थ्', 'आ'], [], ['भ्', 'आ', 'ज्', 'अ', 'प्', 'आ'], ['क्', 'ए'], ['द्', 'अ'], ['इ'], ['व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए'], ['त्', 'आ'], ['प्', 'र्', 'अ'], ['म्', 'अ'], ['ओ'], ['द्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'ए'], ['ट्', 'ई'], ['प्', 'अ'], ['ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ', 'च्', 'इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], []]\n",
    "  separate_words=[]\n",
    "  final_list=[]\n",
    "  for word in range(len(tokens)):\n",
    "\n",
    "      # for i in ranghe(len(tokens[word])):\n",
    "      for i in range(len(tokens[word])):\n",
    "          if(tokens[word][i] in consonants and i+1<=len(tokens[word])-1 and tokens[word][i+1] in consonants):\n",
    "              separate_words.append(tokens[word][i]+'्')\n",
    "              # tokens[word][i]=tokens[word][i]+'्'\n",
    "              # tokens[word].insert(i+1, 'अ')\n",
    "              separate_words.append('अ')\n",
    "\n",
    "\n",
    "          # If the consonant is at the final index\n",
    "          elif(tokens[word][i] in consonants and i==len(tokens[word])-1):\n",
    "              separate_words.append(tokens[word][i]+'्')\n",
    "              separate_words.append('अ')\n",
    "\n",
    "\n",
    "\n",
    "          elif(tokens[word][i] in consonants and i+1<=len(tokens[word])-1 and tokens[word][i+1] not in consonants):\n",
    "              separate_words.append(tokens[word][i]+'्')\n",
    "\n",
    "          elif(tokens[word][i] in matra):\n",
    "              # tokens[word][i]=mapping_dict[lst[word][i]]\n",
    "              separate_words.append(mapping_dict[tokens[word][i]])\n",
    "\n",
    "          elif(tokens[word][i] in vowels):\n",
    "              # tokens[word][i]=mapping_dict[tokens[word][i]]\n",
    "              separate_words.append(tokens[word][i])\n",
    "\n",
    "\n",
    "      final_list.append(separate_words)\n",
    "      separate_words=[]\n",
    "  return final_list\n",
    "\n",
    "unigramCharacterOutput = unigramCharacter(tokens)\n",
    "print(unigramCharacterOutput[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating syllables from the token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJUqcup6j_7J",
    "outputId": "54da86d6-ca42-451f-9dfa-abee721f6797"
   },
   "outputs": [],
   "source": [
    "unigramCharFinal=[]\n",
    "for element in range(len(unigramCharacterOutput)):\n",
    "  temp=\"\"\n",
    "  for i in range(len(unigramCharacterOutput[element])):\n",
    "    temp+=unigramCharacterOutput[element][i]\n",
    "  unigramCharFinal.append(temp)\n",
    "\n",
    "# print(unigramCharFinal)\n",
    "\n",
    "# SYLLABLE GENERATOR\n",
    "def syllableGenerator(unigramCharacterOutput):\n",
    "  tempList=[]\n",
    "  tempListFinal=[]\n",
    "  for sublist in range(len(unigramCharacterOutput)):\n",
    "      # for i in range(len(sublist)):\n",
    "      i=0\n",
    "      # temp=[]\n",
    "      temp=\"\"\n",
    "      total=[]\n",
    "      # while(unigramCharacterOutput[sublist][i] not in vowels):\n",
    "      #     temp.append(unigramCharacterOutput[sublist][i])\n",
    "\n",
    "      for i in range(len(unigramCharacterOutput[sublist])):\n",
    "          if(unigramCharacterOutput[sublist][i] not in vowels or unigramCharacterOutput[sublist][i] in extraSounds):\n",
    "              # temp.append(unigramCharacterOutput[sublist][i])\n",
    "              temp+=unigramCharacterOutput[sublist][i]\n",
    "              ''' FOR TESTING 1\n",
    "              print(\"1\")\n",
    "              print(temp)\n",
    "              '''\n",
    "          else:\n",
    "              # temp.append(unigramCharacterOutput[sublist][i])\n",
    "              if(temp.endswith('्')):\n",
    "                temp=temp[:-1]\n",
    "              if(i==0):\n",
    "                temp+=unigramCharacterOutput[sublist][i]\n",
    "              else:\n",
    "                temp+=rev_dict[unigramCharacterOutput[sublist][i]]\n",
    "              # process(temp)\n",
    "              # Code for process(temp)\n",
    "              ''' FOR TESTING 2\n",
    "              print(\"2\")\n",
    "              size=len(temp)\n",
    "              print(temp)\n",
    "              '''\n",
    "              if(i<=len(unigramCharacterOutput[sublist])-2 and unigramCharacterOutput[sublist][i+1] in vowels):\n",
    "                # temp+=rev_dict[unigramCharacterOutput[sublist][i]]\n",
    "                continue\n",
    "\n",
    "              tempList.append(temp)\n",
    "\n",
    "              temp=\"\"\n",
    "              # temp[size-2]=temp[size-2]+'अ'\n",
    "              # temp[size-2]=temp[size-2]+temp[size-1]\n",
    "              # temp.pop()\n",
    "              # print(temp)\n",
    "              total.append(temp)\n",
    "\n",
    "      if(tempList!=[]):\n",
    "        tempListFinal.append(tempList)\n",
    "      tempList=[]\n",
    "  return tempListFinal\n",
    "\n",
    "print(syllableGenerator(unigramCharacterOutput[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmd7t3IWkEoD"
   },
   "source": [
    "## Cleaning(Preprocessing) Unigram Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9OkFnjvjbiR"
   },
   "source": [
    "front hindi char no -- so rmove it(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मोदी', 'सरकार', 'के', 'पहले', 'कार्य', 'का', 'ल', 'में', 'भी', 'तीन', 'त', 'ला', 'क', 'को', 'लेकर', 'बि', 'ल', 'ल', 'ाया', 'गया', 'था', 'हालांकि', 'तब', 'यह', 'राज्य', 'स', 'भा', 'में', 'पास', 'नहीं', 'हो', 'प', 'ाया', 'था', 'भाजपा', 'के', 'द', 'ि', 'व', 'ंग', 'त', 'ने', 'ता', 'प्र', 'म', 'ो', 'द', 'महा', 'जन', 'की', 'बे', 'टी', 'प', 'ू', 'न', 'म', 'महा', 'जन', 'को', 'सचिव', 'बना', 'या', 'गया', 'है', 'ऐसी', 'स्थित', 'ि', 'में', 'एक', 'न्याय', 'पूर्ण', 'सरकार', 'सा', 'र', '्व', 'जन', 'िक', 'वि', 'त्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'कर', 'ती', 'है', 'कि', 'सं', 'सा', 'ध', 'न', 'ों', 'का', 'आ', 'व', 'ंट', 'न', 'सभी', 'के', 'उप', 'भ', 'ो', 'ग', 'वाले', 'उत्पाद', 'ों', 'की', 'व', '्य', 'व', 'हा', 'र', '्य', 'ता', 'और', 'सम', 'ग्र', 'व', 'ृ', 'ह', 'द', 'आ', 'र्थ', 'िक', 'प्र', 'बंध', 'न', 'न', 'ि', 'ष', '्', 'प', 'क्ष', 'ता', 'के', 'रूप', 'में', 'न्याय', 'को', 'बढ', 'ा', 'ए', 'दिल', 'च', 'स्प', 'है', 'कि', 'डी', 'सी', 'ए', 'च', 'ए', 'ल', 'के', 'च', 'े', 'य', 'र', 'म', 'ै', 'न', 'टी', 'वे', 'ंक', 'टर', 'म', 'न', 'रे', 'ड्ड', 'ी', 'और', 'व', 'ाइ', 'स', 'च', 'े', 'य', 'र', 'म', 'ै', 'न', 'टी', 'वि', 'न', 'ाय', 'क', 'र', 'वि', 'इस', 'बैठक', 'में', 'मौजूद', 'नहीं', 'थे']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "print(processedUnigram[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of Unigram Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'ओ', 'द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'आ'], ['ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'इ'], ['ल्', 'अ'], ['ल्', 'अ'], ['आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ'], ['भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'अ'], ['आ', 'य्', 'आ'], ['थ्', 'आ'], ['भ्', 'आ', 'ज्', 'अ', 'प्', 'आ'], ['क्', 'ए'], ['द्', 'अ'], ['इ'], ['व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए'], ['त्', 'आ'], ['प्', 'र्', 'अ'], ['म्', 'अ'], ['ओ'], ['द्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'ए'], ['ट्', 'ई'], ['प्', 'अ'], ['ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ', 'च्', 'इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], ['ऐ', 'स्', 'ई'], ['स्', 'थ्', 'इ', 'त्', 'अ'], ['इ'], ['म्', 'ए', 'अं'], ['ए', 'क्', 'अ'], ['न्', 'य्', 'आ', 'य्', 'अ'], ['प्', 'ऊ', 'र्', 'ण्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['स्', 'आ'], ['र्', 'अ'], ['व्', 'अ'], ['ज्', 'अ', 'न्', 'अ'], ['इ', 'क्', 'अ'], ['व्', 'इ'], ['त्', 'त्', 'अ'], ['क्', 'आ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'त्', 'ए', 'म्', 'आ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['त्', 'ई'], ['ह्', 'ऐ'], ['क्', 'इ'], ['स्', 'अं'], ['स्', 'आ'], ['ध्', 'अ'], ['न्', 'अ'], ['ओ', 'अं'], ['क्', 'आ'], ['आ'], ['व्', 'अ'], ['अं', 'ट्', 'अ'], ['न्', 'अ'], ['स्', 'अ', 'भ्', 'ई'], ['क्', 'ए'], ['उ', 'प्', 'अ'], ['भ्', 'अ'], ['ओ'], ['ग्', 'अ'], ['व्', 'आ', 'ल्', 'ए'], ['उ', 'त्', 'प्', 'आ', 'द्', 'अ'], ['ओ', 'अं'], ['क्', 'ई'], ['व्', 'अ'], ['य्', 'अ'], ['व्', 'अ'], ['ह्', 'आ'], ['र्', 'अ'], ['य्', 'अ'], ['त्', 'आ'], ['औ', 'र्', 'अ'], ['स्', 'अ', 'म्', 'अ'], ['ग्', 'र्', 'अ'], ['व्', 'अ'], ['ऋ'], ['ह्', 'अ'], ['द्', 'अ'], ['आ'], ['र्', 'थ्', 'अ'], ['इ', 'क्', 'अ'], ['प्', 'र्', 'अ'], ['ब्', 'अं', 'ध्', 'अ'], ['न्', 'अ'], ['न्', 'अ'], ['इ'], ['ष्', 'अ'], [], ['प्', 'अ'], ['क्', 'ष्', 'अ'], ['त्', 'आ'], ['क्', 'ए'], ['र्', 'ऊ', 'प्', 'अ'], ['म्', 'ए', 'अं'], ['न्', 'य्', 'आ', 'य्', 'अ'], ['क्', 'ओ'], ['ब्', 'अ', 'ढ्', 'अ'], ['आ'], ['ए'], ['द्', 'इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'प्', 'अ'], ['ह्', 'ऐ'], ['क्', 'इ'], ['ड्', 'ई'], ['स्', 'ई'], ['ए'], ['च्', 'अ'], ['ए'], ['ल्', 'अ'], ['क्', 'ए'], ['च्', 'अ'], ['ए'], ['य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ'], ['न्', 'अ'], ['ट्', 'ई'], ['व्', 'ए'], ['अं', 'क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['न्', 'अ'], ['र्', 'ए'], ['ड्', 'ड्', 'अ'], ['ई'], ['औ', 'र्', 'अ'], ['व्', 'अ'], ['आ', 'इ'], ['स्', 'अ'], ['च्', 'अ'], ['ए'], ['य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ'], ['न्', 'अ'], ['ट्', 'ई'], ['व्', 'इ'], ['न्', 'अ'], ['आ', 'य्', 'अ'], ['क्', 'अ'], ['र्', 'अ'], ['व्', 'इ'], ['इ', 'स्', 'अ'], ['ब्', 'ऐ', 'ठ्', 'अ', 'क्', 'अ'], ['म्', 'ए', 'अं'], ['म्', 'औ', 'ज्', 'ऊ', 'द्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['थ्', 'ए']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z83iJew4mSHS",
    "outputId": "b959f45e-41da-4e2b-bc78-e2891fa61d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 133, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "generatedSyllables\n",
      "[['मो', 'दी'], ['स', 'र', 'का', 'र'], ['के'], ['प', 'ह', 'ले'], ['का', 'र्य'], ['का'], ['ल'], ['में'], ['भी'], ['ती', 'न'], ['त'], ['ला'], ['क'], ['को'], ['ले', 'क', 'र'], ['बि'], ['ल'], ['ल'], ['आ', 'या'], ['ग', 'या'], ['था'], ['हा', 'लां', 'कि'], ['त', 'ब'], ['य', 'ह'], ['रा', 'ज्य'], ['स'], ['भा'], ['में'], ['पा', 'स'], ['न', 'हीं'], ['हो'], ['प'], ['आ', 'या'], ['था'], ['भा', 'ज', 'पा'], ['के'], ['द'], ['इ'], ['व'], ['अं', 'ग'], ['त'], ['ने'], ['ता'], ['प्र'], ['म'], ['ओ'], ['द'], ['म', 'हा'], ['ज', 'न'], ['की'], ['बे'], ['टी'], ['प'], ['ऊ'], ['न'], ['म'], ['म', 'हा'], ['ज', 'न'], ['को'], ['स', 'चि', 'व'], ['ब', 'ना'], ['या'], ['ग', 'या'], ['है'], ['ऐ', 'सी'], ['स्थि', 'त'], ['इ'], ['में'], ['ए', 'क'], ['न्या', 'य'], ['पू', 'र्ण'], ['स', 'र', 'का', 'र'], ['सा'], ['र'], ['व'], ['ज', 'न'], ['इ', 'क'], ['वि'], ['त्त'], ['का'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स्ते', 'मा', 'ल'], ['क', 'र'], ['ती'], ['है'], ['कि'], ['सं'], ['सा'], ['ध'], ['न'], ['ओं'], ['का'], ['आ'], ['व'], ['अं', 'ट'], ['न'], ['स', 'भी'], ['के'], ['उ', 'प'], ['भ'], ['ओ'], ['ग'], ['वा', 'ले'], ['उ', 'त्पा', 'द'], ['ओं'], ['की'], ['व'], ['य'], ['व'], ['हा'], ['र'], ['य'], ['ता'], ['औ', 'र'], ['स', 'म'], ['ग्र'], ['व'], ['ऋ'], ['ह'], ['द'], ['आ'], ['र्थ'], ['इ', 'क'], ['प्र'], ['बं', 'ध'], ['न'], ['न'], ['इ'], ['ष'], ['प'], ['क्ष'], ['ता'], ['के'], ['रू', 'प'], ['में'], ['न्या', 'य'], ['को'], ['ब', 'ढ'], ['आ'], ['ए'], ['दि', 'ल'], ['च'], ['स्प'], ['है'], ['कि'], ['डी'], ['सी'], ['ए'], ['च'], ['ए'], ['ल'], ['के'], ['च'], ['ए'], ['य'], ['र'], ['म'], ['ऐ'], ['न'], ['टी'], ['वे'], ['अं', 'क'], ['ट', 'र'], ['म'], ['न'], ['रे'], ['ड्ड'], ['ई'], ['औ', 'र'], ['व'], ['आि'], ['स'], ['च'], ['ए'], ['य'], ['र'], ['म'], ['ऐ'], ['न'], ['टी'], ['वि'], ['न'], ['आ', 'य'], ['क'], ['र'], ['वि'], ['इ', 'स'], ['बै', 'ठ', 'क'], ['में'], ['मौ', 'जू', 'द'], ['न', 'हीं'], ['थे']]\n",
      "sortedSyllableCount_desc1\n",
      "{'र': 15, 'न': 15, 'स': 10, 'क': 9, 'य': 8, 'इ': 8, 'व': 8, 'म': 8, 'का': 6, 'प': 6, 'ल': 6, 'आ': 6, 'ए': 6, 'के': 5, 'में': 5, 'त': 5, 'या': 5, 'द': 5, 'ह': 4, 'ग': 4}\n",
      "bi_characterCount1\n",
      "{'र्अ': 18, 'न्अ': 15, 'स्अ': 10, 'य्अ': 10, 'क्अ': 9, 'व्अ': 8, 'म्अ': 8, 'प्अ': 7, 'अह्': 7, 'य्आ': 7, 'अर्': 6, 'क्आ': 6, 'ल्अ': 6, 'त्अ': 6, 'क्ए': 5, 'म्ए': 5, 'एअं': 5, 'आय्': 5, 'द्अ': 5, 'ह्अ': 4}\n",
      "bi_syllableCount1\n",
      "{'जन': 3, 'सर': 2, 'रका': 2, 'कार': 2, 'कर': 2, 'आया': 2, 'गया': 2, 'नहीं': 2, 'महा': 2, 'न्याय': 2, 'इक': 2, 'इस': 2, 'और': 2, 'मोदी': 1, 'पह': 1, 'हले': 1, 'कार्य': 1, 'तीन': 1, 'लेक': 1, 'हालां': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE - vocab size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁मोदी', '▁सरकार', '▁के', '▁पहले', '▁कार्य', 'क', 'ाल', '▁में', '▁भी', '▁तीन', '▁त', 'ला', 'क', '▁को', '▁लेकर', '▁ब', 'िल', '▁ला', 'या', '▁गया', '▁था', ',', '▁हाल', 'ां', 'कि', '▁त', 'ब', '▁यह', '▁राज्य', 'सभा', '▁में', '▁पास', '▁नहीं', '▁हो', '▁पा', 'या', '▁था', '.', '▁भाजपा', '▁के', '▁द', 'िव', 'ंग', 'त', '▁नेता', '▁प्र', 'म', 'ो', 'द', '▁महा', 'जन', '▁की', '▁बे', 'टी', '▁पू', 'न', 'म', '▁महा', 'जन', '▁को', '▁स', 'च', 'िव', '▁बना', 'या', '▁गया', '▁है', '.', '▁ऐ', 'सी', '▁स्थित', 'ि', '▁में', '▁एक', '▁न', '्या', 'य', 'प', 'ूर्', 'ण', '▁सरकार', '▁स', 'ार्', 'व', 'जन', 'िक', '▁व', 'ित', '्त', '▁का', '▁इस', '▁तरह', '▁इस्', 'ते', 'म', 'ाल', '▁कर', 'ती', '▁है', '▁कि', '▁सं', 'सा', 'ध', 'नों', '▁का', '▁आ', 'वं', 'ट', 'न', ',', '▁सभी', '▁के', '▁उप', 'भ', 'ोग', '▁वाले', '▁उत्', 'पा', 'द', 'ों', '▁की', '▁व्य', 'व', 'हार', '्य', 'ता', '▁और', '▁सम', 'ग', '्र', '▁व', 'ृ', 'ह', 'द', '-', 'आ', 'र्थ', 'िक', '▁प्र', 'बंध', 'न', \"▁'\", 'नि', 'ष्', 'प', 'क्ष', 'ता', '▁के', '▁रूप', '▁में', '▁न', '्या', 'य', \"'\", '▁को', '▁बढ़', 'ा', 'ए', '।', '▁द', 'िल', 'च', 'स्प', '▁है', '▁कि', '▁डी', 'सी', 'ए', 'च', 'ए', 'ल', '▁के', '▁चे', 'यर', 'म', 'ैन', '▁टी', '▁व', 'ें', 'क', 'टर', 'म', 'न', '▁रे', 'ड', '्', 'डी', '▁और', '▁व', 'ाइ', 'स', '▁चे', 'यर', 'म', 'ैन', '▁टी', '▁वि', 'ना', 'य', 'क', '▁र', 'वि', '▁इस', '▁बैठ', 'क', '▁में', '▁मौ', 'जूद', '▁नहीं', '▁थे', '।']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load your text data\n",
    "text_data = \"hi_100.txt\"\n",
    "\n",
    "# Train the SentencePiece tokenizer\n",
    "spm.SentencePieceTrainer.train(input=text_data, model_prefix='bpe_model', model_type='bpe', vocab_size=1000)\n",
    "\n",
    "# Load the trained tokenizer\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load('bpe_model.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence=content\n",
    "tokens = bpe.encode_as_pieces(sentence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "\n",
    "lst = []\n",
    "temp=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for BPE, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मोदी', 'सरकार', 'के', 'पहले', 'कार्य', 'क', 'ाल', 'में', 'भी', 'तीन', 'त', 'ला', 'क', 'को', 'लेकर', 'ब', 'िल', 'ला', 'या', 'गया', 'था', 'हाल', 'ां', 'कि', 'त', 'ब', 'यह', 'राज्य', 'सभा', 'में', 'पास', 'नहीं', 'हो', 'पा', 'या', 'था', 'भाजपा', 'के', 'द', 'िव', 'ंग', 'त', 'नेता', 'प्र', 'म', 'ो', 'द', 'महा', 'जन', 'की', 'बे', 'टी', 'पू', 'न', 'म', 'महा', 'जन', 'को', 'स', 'च', 'िव', 'बना', 'या', 'गया', 'है', 'ऐ', 'सी', 'स्थित', 'ि', 'में', 'एक', 'न', '्या', 'य', 'प', 'ूर्', 'ण', 'सरकार', 'स', 'ार्', 'व', 'जन', 'िक', 'व', 'ित', '्त', 'का', 'इस', 'तरह', 'इस्', 'ते', 'म', 'ाल', 'कर', 'ती', 'है', 'कि', 'सं', 'सा', 'ध', 'नों', 'का', 'आ', 'वं', 'ट', 'न', 'सभी', 'के', 'उप', 'भ', 'ोग', 'वाले', 'उत्', 'पा', 'द', 'ों', 'की', 'व्य', 'व', 'हार', '्य', 'ता', 'और', 'सम', 'ग', '्र', 'व', 'ृ', 'ह', 'द', 'आ', 'र्थ', 'िक', 'प्र', 'बंध', 'न', 'नि', 'ष्', 'प', 'क्ष', 'ता', 'के', 'रूप', 'में', 'न', '्या', 'य', 'को', 'बढ', 'ा', 'ए', 'द', 'िल', 'च', 'स्प', 'है', 'कि', 'डी', 'सी', 'ए', 'च', 'ए', 'ल', 'के', 'चे', 'यर', 'म', 'ैन', 'टी', 'व', 'ें', 'क', 'टर', 'म', 'न', 'रे', 'ड', '्', 'डी', 'और', 'व', 'ाइ', 'स', 'चे', 'यर', 'म', 'ैन', 'टी', 'वि', 'ना', 'य', 'क', 'र', 'वि', 'इस', 'बैठ', 'क', 'में', 'मौ', 'जूद', 'नहीं', 'थे']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "# for word in tokens:\n",
    "#   if(word[0]=='_')\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of BPE, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'ओ', 'द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'अ'], ['आ', 'ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'अ'], ['इ', 'ल्', 'अ'], ['ल्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'अ'], ['आ', 'अं'], ['क्', 'इ'], ['त्', 'अ'], ['ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ', 'भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'आ'], ['य्', 'आ'], ['थ्', 'आ'], ['भ्', 'आ', 'ज्', 'अ', 'प्', 'आ'], ['क्', 'ए'], ['द्', 'अ'], ['इ', 'व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए', 'त्', 'आ'], ['प्', 'र्', 'अ'], ['म्', 'अ'], ['ओ'], ['द्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'ए'], ['ट्', 'ई'], ['प्', 'ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ'], ['च्', 'अ'], ['इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], ['ऐ'], ['स्', 'ई'], ['स्', 'थ्', 'इ', 'त्', 'अ'], ['इ'], ['म्', 'ए', 'अं'], ['ए', 'क्', 'अ'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['प्', 'अ'], ['ऊ', 'र्'], ['ण्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['स्', 'अ'], ['आ', 'र्'], ['व्', 'अ'], ['ज्', 'अ', 'न्', 'अ'], ['इ', 'क्', 'अ'], ['व्', 'अ'], ['इ', 'त्', 'अ'], ['त्', 'अ'], ['क्', 'आ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्'], ['त्', 'ए'], ['म्', 'अ'], ['आ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['त्', 'ई'], ['ह्', 'ऐ'], ['क्', 'इ'], ['स्', 'अं'], ['स्', 'आ'], ['ध्', 'अ'], ['न्', 'ओ', 'अं'], ['क्', 'आ'], ['आ'], ['व्', 'अं'], ['ट्', 'अ'], ['न्', 'अ'], ['स्', 'अ', 'भ्', 'ई'], ['क्', 'ए'], ['उ', 'प्', 'अ'], ['भ्', 'अ'], ['ओ', 'ग्', 'अ'], ['व्', 'आ', 'ल्', 'ए'], ['उ', 'त्'], ['प्', 'आ'], ['द्', 'अ'], ['ओ', 'अं'], ['क्', 'ई'], ['व्', 'य्', 'अ'], ['व्', 'अ'], ['ह्', 'आ', 'र्', 'अ'], ['य्', 'अ'], ['त्', 'आ'], ['औ', 'र्', 'अ'], ['स्', 'अ', 'म्', 'अ'], ['ग्', 'अ'], ['र्', 'अ'], ['व्', 'अ'], ['ऋ'], ['ह्', 'अ'], ['द्', 'अ'], ['आ'], ['र्', 'थ्', 'अ'], ['इ', 'क्', 'अ'], ['प्', 'र्', 'अ'], ['ब्', 'अं', 'ध्', 'अ'], ['न्', 'अ'], ['न्', 'इ'], ['ष्'], ['प्', 'अ'], ['क्', 'ष्', 'अ'], ['त्', 'आ'], ['क्', 'ए'], ['र्', 'ऊ', 'प्', 'अ'], ['म्', 'ए', 'अं'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['क्', 'ओ'], ['ब्', 'अ', 'ढ्', 'अ'], ['आ'], ['ए'], ['द्', 'अ'], ['इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'प्', 'अ'], ['ह्', 'ऐ'], ['क्', 'इ'], ['ड्', 'ई'], ['स्', 'ई'], ['ए'], ['च्', 'अ'], ['ए'], ['ल्', 'अ'], ['क्', 'ए'], ['च्', 'ए'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'अ'], ['ए', 'अं'], ['क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['न्', 'अ'], ['र्', 'ए'], ['ड्', 'अ'], [], ['ड्', 'ई'], ['औ', 'र्', 'अ'], ['व्', 'अ'], ['आ', 'इ'], ['स्', 'अ'], ['च्', 'ए'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'इ'], ['न्', 'आ'], ['य्', 'अ'], ['क्', 'अ'], ['र्', 'अ'], ['व्', 'इ'], ['इ', 'स्', 'अ'], ['ब्', 'ऐ', 'ठ्', 'अ'], ['क्', 'अ'], ['म्', 'ए', 'अं'], ['म्', 'औ'], ['ज्', 'ऊ', 'द्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['थ्', 'ए']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 135, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "generatedSyllables\n",
      "[['मो', 'दी'], ['स', 'र', 'का', 'र'], ['के'], ['प', 'ह', 'ले'], ['का', 'र्य'], ['क'], ['आ', 'ल'], ['में'], ['भी'], ['ती', 'न'], ['त'], ['ला'], ['क'], ['को'], ['ले', 'क', 'र'], ['ब'], ['इ', 'ल'], ['ला'], ['या'], ['ग', 'या'], ['था'], ['हा', 'ल'], ['आं'], ['कि'], ['त'], ['ब'], ['य', 'ह'], ['रा', 'ज्य'], ['स', 'भा'], ['में'], ['पा', 'स'], ['न', 'हीं'], ['हो'], ['पा'], ['या'], ['था'], ['भा', 'ज', 'पा'], ['के'], ['द'], ['इ', 'व'], ['अं', 'ग'], ['त'], ['ने', 'ता'], ['प्र'], ['म'], ['ओ'], ['द'], ['म', 'हा'], ['ज', 'न'], ['की'], ['बे'], ['टी'], ['पू'], ['न'], ['म'], ['म', 'हा'], ['ज', 'न'], ['को'], ['स'], ['च'], ['इ', 'व'], ['ब', 'ना'], ['या'], ['ग', 'या'], ['है'], ['ऐ'], ['सी'], ['स्थि', 'त'], ['इ'], ['में'], ['ए', 'क'], ['न'], ['या'], ['य'], ['प'], ['ऊ'], ['ण'], ['स', 'र', 'का', 'र'], ['स'], ['आ'], ['व'], ['ज', 'न'], ['इ', 'क'], ['व'], ['इ', 'त'], ['त'], ['का'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ'], ['ते'], ['म'], ['आ', 'ल'], ['क', 'र'], ['ती'], ['है'], ['कि'], ['सं'], ['सा'], ['ध'], ['नों'], ['का'], ['आ'], ['वं'], ['ट'], ['न'], ['स', 'भी'], ['के'], ['उ', 'प'], ['भ'], ['ओ', 'ग'], ['वा', 'ले'], ['उ'], ['पा'], ['द'], ['ओं'], ['की'], ['व्य'], ['व'], ['हा', 'र'], ['य'], ['ता'], ['औ', 'र'], ['स', 'म'], ['ग'], ['र'], ['व'], ['ऋ'], ['ह'], ['द'], ['आ'], ['र्थ'], ['इ', 'क'], ['प्र'], ['बं', 'ध'], ['न'], ['नि'], ['प'], ['क्ष'], ['ता'], ['के'], ['रू', 'प'], ['में'], ['न'], ['या'], ['य'], ['को'], ['ब', 'ढ'], ['आ'], ['ए'], ['द'], ['इ', 'ल'], ['च'], ['स्प'], ['है'], ['कि'], ['डी'], ['सी'], ['ए'], ['च'], ['ए'], ['ल'], ['के'], ['चे'], ['य', 'र'], ['म'], ['ऐ', 'न'], ['टी'], ['व'], ['एं'], ['क'], ['ट', 'र'], ['म'], ['न'], ['रे'], ['ड'], ['डी'], ['औ', 'र'], ['व'], ['आि'], ['स'], ['चे'], ['य', 'र'], ['म'], ['ऐ', 'न'], ['टी'], ['वि'], ['ना'], ['य'], ['क'], ['र'], ['वि'], ['इ', 'स'], ['बै', 'ठ'], ['क'], ['में'], ['मौ'], ['जू', 'द'], ['न', 'हीं'], ['थे']]\n",
      "sortedSyllableCount_desc1\n",
      "{'र': 15, 'न': 14, 'स': 11, 'इ': 11, 'क': 10, 'म': 9, 'व': 8, 'त': 7, 'या': 7, 'य': 7, 'आ': 6, 'ल': 6, 'द': 6, 'का': 5, 'के': 5, 'प': 5, 'में': 5, 'ग': 5, 'ह': 4, 'ब': 4}\n",
      "bi_characterCount1\n",
      "{'र्अ': 17, 'न्अ': 14, 'स्अ': 11, 'य्अ': 10, 'क्अ': 10, 'म्अ': 9, 'अर्': 8, 'व्अ': 8, 'अह्': 7, 'त्अ': 7, 'य्आ': 7, 'प्अ': 6, 'ल्अ': 6, 'एअं': 6, 'द्अ': 6, 'क्आ': 5, 'आर्': 5, 'क्ए': 5, 'म्ए': 5, 'ग्अ': 5}\n",
      "bi_syllableCount1\n",
      "{'जन': 3, 'सर': 2, 'रका': 2, 'कार': 2, 'आल': 2, 'कर': 2, 'इल': 2, 'गया': 2, 'नहीं': 2, 'इव': 2, 'महा': 2, 'इक': 2, 'इस': 2, 'और': 2, 'यर': 2, 'ऐन': 2, 'मोदी': 1, 'पह': 1, 'हले': 1, 'कार्य': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE, vocab size=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁मोदी', '▁सरकार', '▁के', '▁पहले', '▁कार्य', 'काल', '▁में', '▁भी', '▁तीन', '▁त', 'ला', 'क', '▁को', '▁लेकर', '▁बिल', '▁ला', 'या', '▁गया', '▁था', ',', '▁हालांकि', '▁तब', '▁यह', '▁राज्य', 'सभा', '▁में', '▁पास', '▁नहीं', '▁हो', '▁पा', 'या', '▁था', '.', '▁भाजपा', '▁के', '▁दिव', 'ंग', 'त', '▁नेता', '▁प्र', 'मो', 'द', '▁महा', 'जन', '▁की', '▁बे', 'टी', '▁पू', 'न', 'म', '▁महा', 'जन', '▁को', '▁सचिव', '▁बनाया', '▁गया', '▁है', '.', '▁ऐसी', '▁स्थिति', '▁में', '▁एक', '▁न्या', 'य', 'पूर्ण', '▁सरकार', '▁स', 'ार्', 'व', 'जन', 'िक', '▁वित', '्त', '▁का', '▁इस', '▁तरह', '▁इस्तेमाल', '▁करती', '▁है', '▁कि', '▁सं', 'सा', 'ध', 'नों', '▁का', '▁आ', 'वं', 'टन', ',', '▁सभी', '▁के', '▁उप', 'भ', 'ोग', '▁वाले', '▁उत्पा', 'दों', '▁की', '▁व्यव', 'हार', '्य', 'ता', '▁और', '▁सम', 'ग्र', '▁वृ', 'ह', 'द', '-', 'आ', 'र्थ', 'िक', '▁प्र', 'बंधन', \"▁'\", 'नि', 'ष्', 'पक्ष', 'ता', '▁के', '▁रूप', '▁में', '▁न्या', 'य', \"'\", '▁को', '▁बढ़ा', 'ए', '।', '▁दिल', 'च', 'स्प', '▁है', '▁कि', '▁डी', 'सी', 'ए', 'च', 'एल', '▁के', '▁चे', 'यर', 'म', 'ैन', '▁टी', '▁व', 'ें', 'क', 'टर', 'मन', '▁रे', 'ड', '्', 'डी', '▁और', '▁व', 'ाइ', 'स', '▁चे', 'यर', 'म', 'ैन', '▁टी', '▁वि', 'ना', 'य', 'क', '▁रवि', '▁इस', '▁बैठक', '▁में', '▁मौजूद', '▁नहीं', '▁थे', '।']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load your text data\n",
    "text_data = \"hi_100.txt\"\n",
    "\n",
    "# Train the SentencePiece tokenizer\n",
    "spm.SentencePieceTrainer.train(input=text_data, model_prefix='bpe_model', model_type='bpe', vocab_size=2000)\n",
    "\n",
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_model.model')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence=content\n",
    "tokens = sp.encode_as_pieces(sentence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for BPE, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मोदी', 'सरकार', 'के', 'पहले', 'कार्य', 'काल', 'में', 'भी', 'तीन', 'त', 'ला', 'क', 'को', 'लेकर', 'बिल', 'ला', 'या', 'गया', 'था', 'हालांकि', 'तब', 'यह', 'राज्य', 'सभा', 'में', 'पास', 'नहीं', 'हो', 'पा', 'या', 'था', 'भाजपा', 'के', 'दिव', 'ंग', 'त', 'नेता', 'प्र', 'मो', 'द', 'महा', 'जन', 'की', 'बे', 'टी', 'पू', 'न', 'म', 'महा', 'जन', 'को', 'सचिव', 'बनाया', 'गया', 'है', 'ऐसी', 'स्थिति', 'में', 'एक', 'न्या', 'य', 'पूर्ण', 'सरकार', 'स', 'ार्', 'व', 'जन', 'िक', 'वित', '्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'सं', 'सा', 'ध', 'नों', 'का', 'आ', 'वं', 'टन', 'सभी', 'के', 'उप', 'भ', 'ोग', 'वाले', 'उत्पा', 'दों', 'की', 'व्यव', 'हार', '्य', 'ता', 'और', 'सम', 'ग्र', 'वृ', 'ह', 'द', 'आ', 'र्थ', 'िक', 'प्र', 'बंधन', 'नि', 'ष्', 'पक्ष', 'ता', 'के', 'रूप', 'में', 'न्या', 'य', 'को', 'बढा', 'ए', 'दिल', 'च', 'स्प', 'है', 'कि', 'डी', 'सी', 'ए', 'च', 'एल', 'के', 'चे', 'यर', 'म', 'ैन', 'टी', 'व', 'ें', 'क', 'टर', 'मन', 'रे', 'ड', '्', 'डी', 'और', 'व', 'ाइ', 'स', 'चे', 'यर', 'म', 'ैन', 'टी', 'वि', 'ना', 'य', 'क', 'रवि', 'इस', 'बैठक', 'में', 'मौजूद', 'नहीं', 'थे']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of BPE, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'ओ', 'द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'आ', 'ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'इ', 'ल्', 'अ'], ['ल्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ', 'भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'आ'], ['य्', 'आ'], ['थ्', 'आ'], ['भ्', 'आ', 'ज्', 'अ', 'प्', 'आ'], ['क्', 'ए'], ['द्', 'इ', 'व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए', 'त्', 'आ'], ['प्', 'र्', 'अ'], ['म्', 'ओ'], ['द्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'ए'], ['ट्', 'ई'], ['प्', 'ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ', 'च्', 'इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], ['ऐ', 'स्', 'ई'], ['स्', 'थ्', 'इ', 'त्', 'इ'], ['म्', 'ए', 'अं'], ['ए', 'क्', 'अ'], ['न्', 'य्', 'आ'], ['य्', 'अ'], ['प्', 'ऊ', 'र्', 'ण्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['स्', 'अ'], ['आ', 'र्'], ['व्', 'अ'], ['ज्', 'अ', 'न्', 'अ'], ['इ', 'क्', 'अ'], ['व्', 'इ', 'त्', 'अ'], ['त्', 'अ'], ['क्', 'आ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'त्', 'ए', 'म्', 'आ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'त्', 'ई'], ['ह्', 'ऐ'], ['क्', 'इ'], ['स्', 'अं'], ['स्', 'आ'], ['ध्', 'अ'], ['न्', 'ओ', 'अं'], ['क्', 'आ'], ['आ'], ['व्', 'अं'], ['ट्', 'अ', 'न्', 'अ'], ['स्', 'अ', 'भ्', 'ई'], ['क्', 'ए'], ['उ', 'प्', 'अ'], ['भ्', 'अ'], ['ओ', 'ग्', 'अ'], ['व्', 'आ', 'ल्', 'ए'], ['उ', 'त्', 'प्', 'आ'], ['द्', 'ओ', 'अं'], ['क्', 'ई'], ['व्', 'य्', 'अ', 'व्', 'अ'], ['ह्', 'आ', 'र्', 'अ'], ['य्', 'अ'], ['त्', 'आ'], ['औ', 'र्', 'अ'], ['स्', 'अ', 'म्', 'अ'], ['ग्', 'र्', 'अ'], ['व्', 'ऋ'], ['ह्', 'अ'], ['द्', 'अ'], ['आ'], ['र्', 'थ्', 'अ'], ['इ', 'क्', 'अ'], ['प्', 'र्', 'अ'], ['ब्', 'अं', 'ध्', 'अ', 'न्', 'अ'], ['न्', 'इ'], ['ष्'], ['प्', 'अ', 'क्', 'ष्', 'अ'], ['त्', 'आ'], ['क्', 'ए'], ['र्', 'ऊ', 'प्', 'अ'], ['म्', 'ए', 'अं'], ['न्', 'य्', 'आ'], ['य्', 'अ'], ['क्', 'ओ'], ['ब्', 'अ', 'ढ्', 'आ'], ['ए'], ['द्', 'इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'प्', 'अ'], ['ह्', 'ऐ'], ['क्', 'इ'], ['ड्', 'ई'], ['स्', 'ई'], ['ए'], ['च्', 'अ'], ['ए', 'ल्', 'अ'], ['क्', 'ए'], ['च्', 'ए'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'अ'], ['ए', 'अं'], ['क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['र्', 'ए'], ['ड्', 'अ'], [], ['ड्', 'ई'], ['औ', 'र्', 'अ'], ['व्', 'अ'], ['आ', 'इ'], ['स्', 'अ'], ['च्', 'ए'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'इ'], ['न्', 'आ'], ['य्', 'अ'], ['क्', 'अ'], ['र्', 'अ', 'व्', 'इ'], ['इ', 'स्', 'अ'], ['ब्', 'ऐ', 'ठ्', 'अ', 'क्', 'अ'], ['म्', 'ए', 'अं'], ['म्', 'औ', 'ज्', 'ऊ', 'द्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['थ्', 'ए']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 118, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "generatedSyllables\n",
      "[['मो', 'दी'], ['स', 'र', 'का', 'र'], ['के'], ['प', 'ह', 'ले'], ['का', 'र्य'], ['का', 'ल'], ['में'], ['भी'], ['ती', 'न'], ['त'], ['ला'], ['क'], ['को'], ['ले', 'क', 'र'], ['बि', 'ल'], ['ला'], ['या'], ['ग', 'या'], ['था'], ['हा', 'लां', 'कि'], ['त', 'ब'], ['य', 'ह'], ['रा', 'ज्य'], ['स', 'भा'], ['में'], ['पा', 'स'], ['न', 'हीं'], ['हो'], ['पा'], ['या'], ['था'], ['भा', 'ज', 'पा'], ['के'], ['दि', 'व'], ['अं', 'ग'], ['त'], ['ने', 'ता'], ['प्र'], ['मो'], ['द'], ['म', 'हा'], ['ज', 'न'], ['की'], ['बे'], ['टी'], ['पू'], ['न'], ['म'], ['म', 'हा'], ['ज', 'न'], ['को'], ['स', 'चि', 'व'], ['ब', 'ना', 'या'], ['ग', 'या'], ['है'], ['ऐ', 'सी'], ['स्थि', 'ति'], ['में'], ['ए', 'क'], ['न्या'], ['य'], ['पू', 'र्ण'], ['स', 'र', 'का', 'र'], ['स'], ['आ'], ['व'], ['ज', 'न'], ['इ', 'क'], ['वि', 'त'], ['त'], ['का'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स्ते', 'मा', 'ल'], ['क', 'र', 'ती'], ['है'], ['कि'], ['सं'], ['सा'], ['ध'], ['नों'], ['का'], ['आ'], ['वं'], ['ट', 'न'], ['स', 'भी'], ['के'], ['उ', 'प'], ['भ'], ['ओ', 'ग'], ['वा', 'ले'], ['उ', 'त्पा'], ['दों'], ['की'], ['व्य', 'व'], ['हा', 'र'], ['य'], ['ता'], ['औ', 'र'], ['स', 'म'], ['ग्र'], ['व ृ'], ['ह'], ['द'], ['आ'], ['र्थ'], ['इ', 'क'], ['प्र'], ['बं', 'ध', 'न'], ['नि'], ['प', 'क्ष'], ['ता'], ['के'], ['रू', 'प'], ['में'], ['न्या'], ['य'], ['को'], ['ब', 'ढा'], ['ए'], ['दि', 'ल'], ['च'], ['स्प'], ['है'], ['कि'], ['डी'], ['सी'], ['ए'], ['च'], ['ए', 'ल'], ['के'], ['चे'], ['य', 'र'], ['म'], ['ऐ', 'न'], ['टी'], ['व'], ['एं'], ['क'], ['ट', 'र'], ['म', 'न'], ['रे'], ['ड'], ['डी'], ['औ', 'र'], ['व'], ['आि'], ['स'], ['चे'], ['य', 'र'], ['म'], ['ऐ', 'न'], ['टी'], ['वि'], ['ना'], ['य'], ['क'], ['र', 'वि'], ['इ', 'स'], ['बै', 'ठ', 'क'], ['में'], ['मौ', 'जू', 'द'], ['न', 'हीं'], ['थे']]\n",
      "sortedSyllableCount_desc1\n",
      "{'र': 14, 'न': 12, 'स': 11, 'क': 9, 'य': 7, 'म': 7, 'का': 6, 'त': 6, 'व': 6, 'के': 5, 'ल': 5, 'में': 5, 'या': 5, 'इ': 5, 'प': 4, 'ह': 4, 'ग': 4, 'हा': 4, 'ज': 4, 'ए': 4}\n",
      "bi_characterCount1\n",
      "{'र्अ': 17, 'न्अ': 12, 'स्अ': 11, 'य्अ': 10, 'क्अ': 9, 'अर्': 8, 'अह्': 7, 'य्आ': 7, 'म्अ': 7, 'अन्': 7, 'क्आ': 6, 'एअं': 6, 'त्अ': 6, 'व्अ': 6, 'आर्': 5, 'क्ए': 5, 'प्अ': 5, 'ल्अ': 5, 'म्ए': 5, 'अक्': 4}\n",
      "bi_syllableCount1\n",
      "{'जन': 3, 'सर': 2, 'रका': 2, 'कार': 2, 'कर': 2, 'गया': 2, 'नहीं': 2, 'महा': 2, 'इक': 2, 'इस': 2, 'और': 2, 'यर': 2, 'ऐन': 2, 'मोदी': 1, 'पह': 1, 'हले': 1, 'कार्य': 1, 'काल': 1, 'तीन': 1, 'लेक': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mBERT - vocab size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['म', '##ो', '##दी', 'सरकार', 'के', 'पहले', 'कार्य', '##काल', 'में', 'भी', 'तीन', 'त', '##ला', '##क', 'को', 'लेकर', 'ब', '##िल', 'ला', '##या', 'गया', 'था', ',', 'हालांकि', 'तब', 'यह', 'राज्य', '##स', '##भा', 'में', 'पास', 'नहीं', 'हो', 'पाया', 'था', '.', 'भ', '##ाज', '##पा', 'के', 'द', '##िव', '##ंग', '##त', 'ने', '##ता', 'प', '##्रम', '##ो', '##द', 'म', '##हा', '##जन', 'की', 'ब', '##ेट', '##ी', 'पू', '##न', '##म', 'म', '##हा', '##जन', 'को', 'स', '##च', '##िव', 'बनाया', 'गया', 'है', '.', 'ऐसी', 'स्थिति', 'में', 'एक', 'न', '##्या', '##य', '##पूर्ण', 'सरकार', 'सार्वजनिक', 'वि', '##त', '##्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'सं', '##सा', '##धन', '##ों', 'का', 'आ', '##व', '##ंट', '##न', ',', 'सभी', 'के', 'उ', '##प', '##भ', '##ोग', 'वाले', 'उ', '##त', '##्', '##पा', '##द', '##ों', 'की', 'व', '##्य', '##व', '##हार', '##्य', '##ता', 'और', 'स', '##म', '##ग', '##्र', 'व', '##ृ', '##ह', '##द', '-', 'आर्थिक', 'प', '##्र', '##बंध', '##न', \"'\", 'न', '##ि', '##ष', '##्', '##प', '##क्ष', '##ता', 'के', 'रूप', 'में', 'न', '##्या', '##य', \"'\", 'को', 'ब', '##ढ़', '##ा', '##ए', '।', 'द', '##िल', '##च', '##स', '##्', '##प', 'है', 'कि', 'डी', '##सी', '##ए', '##च', '##ए', '##ल', 'के', 'च', '##ेय', '##र', '##म', '##ैन', 'टी', 'वे', '##ंक', '##टर', '##मन', 'र', '##ेड', '##्ड', '##ी', 'और', 'वा', '##इ', '##स', 'च', '##ेय', '##र', '##म', '##ैन', 'टी', 'वि', '##ना', '##यक', 'र', '##वि', 'इस', 'ब', '##ै', '##ठ', '##क', 'में', 'म', '##ौ', '##जूद', 'नहीं', 'थे', '।']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', max_length=1000)\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence =content\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for mBERT, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['म', 'ो', 'दी', 'सरकार', 'के', 'पहले', 'कार्य', 'काल', 'में', 'भी', 'तीन', 'त', 'ला', 'क', 'को', 'लेकर', 'ब', 'िल', 'ला', 'या', 'गया', 'था', 'हालांकि', 'तब', 'यह', 'राज्य', 'स', 'भा', 'में', 'पास', 'नहीं', 'हो', 'पाया', 'था', 'भ', 'ाज', 'पा', 'के', 'द', 'िव', 'ंग', 'त', 'ने', 'ता', 'प', '्रम', 'ो', 'द', 'म', 'हा', 'जन', 'की', 'ब', 'ेट', 'ी', 'पू', 'न', 'म', 'म', 'हा', 'जन', 'को', 'स', 'च', 'िव', 'बनाया', 'गया', 'है', 'ऐसी', 'स्थिति', 'में', 'एक', 'न', '्या', 'य', 'पूर्ण', 'सरकार', 'सार्वजनिक', 'वि', 'त', '्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'सं', 'सा', 'धन', 'ों', 'का', 'आ', 'व', 'ंट', 'न', 'सभी', 'के', 'उ', 'प', 'भ', 'ोग', 'वाले', 'उ', 'त', '्', 'पा', 'द', 'ों', 'की', 'व', '्य', 'व', 'हार', '्य', 'ता', 'और', 'स', 'म', 'ग', '्र', 'व', 'ृ', 'ह', 'द', 'आर्थिक', 'प', '्र', 'बंध', 'न', 'न', 'ि', 'ष', '्', 'प', 'क्ष', 'ता', 'के', 'रूप', 'में', 'न', '्या', 'य', 'को', 'ब', 'ढ', 'ा', 'ए', 'द', 'िल', 'च', 'स', '्', 'प', 'है', 'कि', 'डी', 'सी', 'ए', 'च', 'ए', 'ल', 'के', 'च', 'ेय', 'र', 'म', 'ैन', 'टी', 'वे', 'ंक', 'टर', 'मन', 'र', 'ेड', '्ड', 'ी', 'और', 'वा', 'इ', 'स', 'च', 'ेय', 'र', 'म', 'ैन', 'टी', 'वि', 'ना', 'यक', 'र', 'वि', 'इस', 'ब', 'ै', 'ठ', 'क', 'में', 'म', 'ौ', 'जूद', 'नहीं', 'थे']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of mBERT, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'अ'], ['ओ'], ['द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'आ', 'ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'अ'], ['इ', 'ल्', 'अ'], ['ल्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ'], ['भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'आ', 'य्', 'आ'], ['थ्', 'आ'], ['भ्', 'अ'], ['आ', 'ज्', 'अ'], ['प्', 'आ'], ['क्', 'ए'], ['द्', 'अ'], ['इ', 'व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए'], ['त्', 'आ'], ['प्', 'अ'], ['र्', 'अ', 'म्', 'अ'], ['ओ'], ['द्', 'अ'], ['म्', 'अ'], ['ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'अ'], ['ए', 'ट्', 'अ'], ['ई'], ['प्', 'ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ'], ['ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ'], ['च्', 'अ'], ['इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], ['ऐ', 'स्', 'ई'], ['स्', 'थ्', 'इ', 'त्', 'इ'], ['म्', 'ए', 'अं'], ['ए', 'क्', 'अ'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['प्', 'ऊ', 'र्', 'ण्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['स्', 'आ', 'र्', 'व्', 'अ', 'ज्', 'अ', 'न्', 'इ', 'क्', 'अ'], ['व्', 'इ'], ['त्', 'अ'], ['त्', 'अ'], ['क्', 'आ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'त्', 'ए', 'म्', 'आ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'त्', 'ई'], ['ह्', 'ऐ'], ['क्', 'इ'], ['स्', 'अं'], ['स्', 'आ'], ['ध्', 'अ', 'न्', 'अ'], ['ओ', 'अं'], ['क्', 'आ'], ['आ'], ['व्', 'अ'], ['अं', 'ट्', 'अ'], ['न्', 'अ'], ['स्', 'अ', 'भ्', 'ई'], ['क्', 'ए'], ['उ'], ['प्', 'अ'], ['भ्', 'अ'], ['ओ', 'ग्', 'अ'], ['व्', 'आ', 'ल्', 'ए'], ['उ'], ['त्', 'अ'], [], ['प्', 'आ'], ['द्', 'अ'], ['ओ', 'अं'], ['क्', 'ई'], ['व्', 'अ'], ['य्', 'अ'], ['व्', 'अ'], ['ह्', 'आ', 'र्', 'अ'], ['य्', 'अ'], ['त्', 'आ'], ['औ', 'र्', 'अ'], ['स्', 'अ'], ['म्', 'अ'], ['ग्', 'अ'], ['र्', 'अ'], ['व्', 'अ'], ['ऋ'], ['ह्', 'अ'], ['द्', 'अ'], ['आ', 'र्', 'थ्', 'इ', 'क्', 'अ'], ['प्', 'अ'], ['र्', 'अ'], ['ब्', 'अं', 'ध्', 'अ'], ['न्', 'अ'], ['न्', 'अ'], ['इ'], ['ष्', 'अ'], [], ['प्', 'अ'], ['क्', 'ष्', 'अ'], ['त्', 'आ'], ['क्', 'ए'], ['र्', 'ऊ', 'प्', 'अ'], ['म्', 'ए', 'अं'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['क्', 'ओ'], ['ब्', 'अ'], ['ढ्', 'अ'], ['आ'], ['ए'], ['द्', 'अ'], ['इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'अ'], [], ['प्', 'अ'], ['ह्', 'ऐ'], ['क्', 'इ'], ['ड्', 'ई'], ['स्', 'ई'], ['ए'], ['च्', 'अ'], ['ए'], ['ल्', 'अ'], ['क्', 'ए'], ['च्', 'अ'], ['ए', 'य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'ए'], ['अं', 'क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['र्', 'अ'], ['ए', 'ड्', 'अ'], ['ड्', 'अ'], ['ई'], ['औ', 'र्', 'अ'], ['व्', 'आ'], ['इ'], ['स्', 'अ'], ['च्', 'अ'], ['ए', 'य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'इ'], ['न्', 'आ'], ['य्', 'अ', 'क्', 'अ'], ['र्', 'अ'], ['व्', 'इ'], ['इ', 'स्', 'अ'], ['ब्', 'अ'], ['ऐ'], ['ठ्', 'अ'], ['क्', 'अ'], ['म्', 'ए', 'अं'], ['म्', 'अ'], ['औ'], ['ज्', 'ऊ', 'द्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['थ्', 'ए']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 143, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "generatedSyllables\n",
      "[['म'], ['ओ'], ['दी'], ['स', 'र', 'का', 'र'], ['के'], ['प', 'ह', 'ले'], ['का', 'र्य'], ['का', 'ल'], ['में'], ['भी'], ['ती', 'न'], ['त'], ['ला'], ['क'], ['को'], ['ले', 'क', 'र'], ['ब'], ['इ', 'ल'], ['ला'], ['या'], ['ग', 'या'], ['था'], ['हा', 'लां', 'कि'], ['त', 'ब'], ['य', 'ह'], ['रा', 'ज्य'], ['स'], ['भा'], ['में'], ['पा', 'स'], ['न', 'हीं'], ['हो'], ['पा', 'या'], ['था'], ['भ'], ['आ', 'ज'], ['पा'], ['के'], ['द'], ['इ', 'व'], ['अं', 'ग'], ['त'], ['ने'], ['ता'], ['प'], ['र', 'म'], ['ओ'], ['द'], ['म'], ['हा'], ['ज', 'न'], ['की'], ['ब'], ['ए', 'ट'], ['ई'], ['पू'], ['न'], ['म'], ['म'], ['हा'], ['ज', 'न'], ['को'], ['स'], ['च'], ['इ', 'व'], ['ब', 'ना', 'या'], ['ग', 'या'], ['है'], ['ऐ', 'सी'], ['स्थि', 'ति'], ['में'], ['ए', 'क'], ['न'], ['या'], ['य'], ['पू', 'र्ण'], ['स', 'र', 'का', 'र'], ['सा', 'र्व', 'ज', 'नि', 'क'], ['वि'], ['त'], ['त'], ['का'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स्ते', 'मा', 'ल'], ['क', 'र', 'ती'], ['है'], ['कि'], ['सं'], ['सा'], ['ध', 'न'], ['ओं'], ['का'], ['आ'], ['व'], ['अं', 'ट'], ['न'], ['स', 'भी'], ['के'], ['उ'], ['प'], ['भ'], ['ओ', 'ग'], ['वा', 'ले'], ['उ'], ['त'], ['पा'], ['द'], ['ओं'], ['की'], ['व'], ['य'], ['व'], ['हा', 'र'], ['य'], ['ता'], ['औ', 'र'], ['स'], ['म'], ['ग'], ['र'], ['व'], ['ऋ'], ['ह'], ['द'], ['आ', 'र्थि', 'क'], ['प'], ['र'], ['बं', 'ध'], ['न'], ['न'], ['इ'], ['ष'], ['प'], ['क्ष'], ['ता'], ['के'], ['रू', 'प'], ['में'], ['न'], ['या'], ['य'], ['को'], ['ब'], ['ढ'], ['आ'], ['ए'], ['द'], ['इ', 'ल'], ['च'], ['स'], ['प'], ['है'], ['कि'], ['डी'], ['सी'], ['ए'], ['च'], ['ए'], ['ल'], ['के'], ['च'], ['ए', 'य'], ['र'], ['म'], ['ऐ', 'न'], ['टी'], ['वे'], ['अं', 'क'], ['ट', 'र'], ['म', 'न'], ['र'], ['ए', 'ड'], ['ड'], ['ई'], ['औ', 'र'], ['वा'], ['इ'], ['स'], ['च'], ['ए', 'य'], ['र'], ['म'], ['ऐ', 'न'], ['टी'], ['वि'], ['ना'], ['य', 'क'], ['र'], ['वि'], ['इ', 'स'], ['ब'], ['ऐ'], ['ठ'], ['क'], ['में'], ['म'], ['औ'], ['जू', 'द'], ['न', 'हीं'], ['थे']]\n",
      "sortedSyllableCount_desc1\n",
      "{'र': 18, 'न': 15, 'स': 11, 'म': 10, 'क': 9, 'इ': 9, 'य': 8, 'ए': 8, 'प': 7, 'त': 7, 'या': 7, 'का': 6, 'ब': 6, 'द': 6, 'व': 6, 'के': 5, 'ल': 5, 'में': 5, 'ग': 5, 'च': 5}\n",
      "bi_characterCount1\n",
      "{'र्अ': 18, 'न्अ': 15, 'स्अ': 11, 'म्अ': 10, 'य्अ': 10, 'क्अ': 9, 'प्अ': 7, 'त्अ': 7, 'य्आ': 7, 'व्अ': 7, 'अर्': 6, 'क्आ': 6, 'आर्': 6, 'ब्अ': 6, 'द्अ': 6, 'अन्': 6, 'क्ए': 5, 'अह्': 5, 'ल्अ': 5, 'म्ए': 5}\n",
      "bi_syllableCount1\n",
      "{'सर': 2, 'रका': 2, 'कार': 2, 'कर': 2, 'इल': 2, 'गया': 2, 'नहीं': 2, 'इव': 2, 'जन': 2, 'इस': 2, 'और': 2, 'एय': 2, 'ऐन': 2, 'पह': 1, 'हले': 1, 'कार्य': 1, 'काल': 1, 'तीन': 1, 'लेक': 1, 'हालां': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mBERT - vocab size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['म', '##ो', '##दी', 'सरकार', 'के', 'पहले', 'कार्य', '##काल', 'में', 'भी', 'तीन', 'त', '##ला', '##क', 'को', 'लेकर', 'ब', '##िल', 'ला', '##या', 'गया', 'था', ',', 'हालांकि', 'तब', 'यह', 'राज्य', '##स', '##भा', 'में', 'पास', 'नहीं', 'हो', 'पाया', 'था', '.', 'भ', '##ाज', '##पा', 'के', 'द', '##िव', '##ंग', '##त', 'ने', '##ता', 'प', '##्रम', '##ो', '##द', 'म', '##हा', '##जन', 'की', 'ब', '##ेट', '##ी', 'पू', '##न', '##म', 'म', '##हा', '##जन', 'को', 'स', '##च', '##िव', 'बनाया', 'गया', 'है', '.', 'ऐसी', 'स्थिति', 'में', 'एक', 'न', '##्या', '##य', '##पूर्ण', 'सरकार', 'सार्वजनिक', 'वि', '##त', '##्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'सं', '##सा', '##धन', '##ों', 'का', 'आ', '##व', '##ंट', '##न', ',', 'सभी', 'के', 'उ', '##प', '##भ', '##ोग', 'वाले', 'उ', '##त', '##्', '##पा', '##द', '##ों', 'की', 'व', '##्य', '##व', '##हार', '##्य', '##ता', 'और', 'स', '##म', '##ग', '##्र', 'व', '##ृ', '##ह', '##द', '-', 'आर्थिक', 'प', '##्र', '##बंध', '##न', \"'\", 'न', '##ि', '##ष', '##्', '##प', '##क्ष', '##ता', 'के', 'रूप', 'में', 'न', '##्या', '##य', \"'\", 'को', 'ब', '##ढ़', '##ा', '##ए', '।', 'द', '##िल', '##च', '##स', '##्', '##प', 'है', 'कि', 'डी', '##सी', '##ए', '##च', '##ए', '##ल', 'के', 'च', '##ेय', '##र', '##म', '##ैन', 'टी', 'वे', '##ंक', '##टर', '##मन', 'र', '##ेड', '##्ड', '##ी', 'और', 'वा', '##इ', '##स', 'च', '##ेय', '##र', '##म', '##ैन', 'टी', 'वि', '##ना', '##यक', 'र', '##वि', 'इस', 'ब', '##ै', '##ठ', '##क', 'में', 'म', '##ौ', '##जूद', 'नहीं', 'थे', '।']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', max_length=2000)\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence = content\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for mBERT, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['म', 'ो', 'दी', 'सरकार', 'के', 'पहले', 'कार्य', 'काल', 'में', 'भी', 'तीन', 'त', 'ला', 'क', 'को', 'लेकर', 'ब', 'िल', 'ला', 'या', 'गया', 'था', 'हालांकि', 'तब', 'यह', 'राज्य', 'स', 'भा', 'में', 'पास', 'नहीं', 'हो', 'पाया', 'था', 'भ', 'ाज', 'पा', 'के', 'द', 'िव', 'ंग', 'त', 'ने', 'ता', 'प', '्रम', 'ो', 'द', 'म', 'हा', 'जन', 'की', 'ब', 'ेट', 'ी', 'पू', 'न', 'म', 'म', 'हा', 'जन', 'को', 'स', 'च', 'िव', 'बनाया', 'गया', 'है', 'ऐसी', 'स्थिति', 'में', 'एक', 'न', '्या', 'य', 'पूर्ण', 'सरकार', 'सार्वजनिक', 'वि', 'त', '्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'सं', 'सा', 'धन', 'ों', 'का', 'आ', 'व', 'ंट', 'न', 'सभी', 'के', 'उ', 'प', 'भ', 'ोग', 'वाले', 'उ', 'त', '्', 'पा', 'द', 'ों', 'की', 'व', '्य', 'व', 'हार', '्य', 'ता', 'और', 'स', 'म', 'ग', '्र', 'व', 'ृ', 'ह', 'द', 'आर्थिक', 'प', '्र', 'बंध', 'न', 'न', 'ि', 'ष', '्', 'प', 'क्ष', 'ता', 'के', 'रूप', 'में', 'न', '्या', 'य', 'को', 'ब', 'ढ', 'ा', 'ए', 'द', 'िल', 'च', 'स', '्', 'प', 'है', 'कि', 'डी', 'सी', 'ए', 'च', 'ए', 'ल', 'के', 'च', 'ेय', 'र', 'म', 'ैन', 'टी', 'वे', 'ंक', 'टर', 'मन', 'र', 'ेड', '्ड', 'ी', 'और', 'वा', 'इ', 'स', 'च', 'ेय', 'र', 'म', 'ैन', 'टी', 'वि', 'ना', 'यक', 'र', 'वि', 'इस', 'ब', 'ै', 'ठ', 'क', 'में', 'म', 'ौ', 'जूद', 'नहीं', 'थे']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of mBERT, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'अ'], ['ओ'], ['द्', 'ई'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['क्', 'ए'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'ए'], ['क्', 'आ', 'र्', 'य्', 'अ'], ['क्', 'आ', 'ल्', 'अ'], ['म्', 'ए', 'अं'], ['भ्', 'ई'], ['त्', 'ई', 'न्', 'अ'], ['त्', 'अ'], ['ल्', 'आ'], ['क्', 'अ'], ['क्', 'ओ'], ['ल्', 'ए', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'अ'], ['इ', 'ल्', 'अ'], ['ल्', 'आ'], ['य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['थ्', 'आ'], ['ह्', 'आ', 'ल्', 'आ', 'अं', 'क्', 'इ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'आ', 'ज्', 'य्', 'अ'], ['स्', 'अ'], ['भ्', 'आ'], ['म्', 'ए', 'अं'], ['प्', 'आ', 'स्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['ह्', 'ओ'], ['प्', 'आ', 'य्', 'आ'], ['थ्', 'आ'], ['भ्', 'अ'], ['आ', 'ज्', 'अ'], ['प्', 'आ'], ['क्', 'ए'], ['द्', 'अ'], ['इ', 'व्', 'अ'], ['अं', 'ग्', 'अ'], ['त्', 'अ'], ['न्', 'ए'], ['त्', 'आ'], ['प्', 'अ'], ['र्', 'अ', 'म्', 'अ'], ['ओ'], ['द्', 'अ'], ['म्', 'अ'], ['ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ई'], ['ब्', 'अ'], ['ए', 'ट्', 'अ'], ['ई'], ['प्', 'ऊ'], ['न्', 'अ'], ['म्', 'अ'], ['म्', 'अ'], ['ह्', 'आ'], ['ज्', 'अ', 'न्', 'अ'], ['क्', 'ओ'], ['स्', 'अ'], ['च्', 'अ'], ['इ', 'व्', 'अ'], ['ब्', 'अ', 'न्', 'आ', 'य्', 'आ'], ['ग्', 'अ', 'य्', 'आ'], ['ह्', 'ऐ'], ['ऐ', 'स्', 'ई'], ['स्', 'थ्', 'इ', 'त्', 'इ'], ['म्', 'ए', 'अं'], ['ए', 'क्', 'अ'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['प्', 'ऊ', 'र्', 'ण्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'क्', 'आ', 'र्', 'अ'], ['स्', 'आ', 'र्', 'व्', 'अ', 'ज्', 'अ', 'न्', 'इ', 'क्', 'अ'], ['व्', 'इ'], ['त्', 'अ'], ['त्', 'अ'], ['क्', 'आ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'त्', 'ए', 'म्', 'आ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'त्', 'ई'], ['ह्', 'ऐ'], ['क्', 'इ'], ['स्', 'अं'], ['स्', 'आ'], ['ध्', 'अ', 'न्', 'अ'], ['ओ', 'अं'], ['क्', 'आ'], ['आ'], ['व्', 'अ'], ['अं', 'ट्', 'अ'], ['न्', 'अ'], ['स्', 'अ', 'भ्', 'ई'], ['क्', 'ए'], ['उ'], ['प्', 'अ'], ['भ्', 'अ'], ['ओ', 'ग्', 'अ'], ['व्', 'आ', 'ल्', 'ए'], ['उ'], ['त्', 'अ'], [], ['प्', 'आ'], ['द्', 'अ'], ['ओ', 'अं'], ['क्', 'ई'], ['व्', 'अ'], ['य्', 'अ'], ['व्', 'अ'], ['ह्', 'आ', 'र्', 'अ'], ['य्', 'अ'], ['त्', 'आ'], ['औ', 'र्', 'अ'], ['स्', 'अ'], ['म्', 'अ'], ['ग्', 'अ'], ['र्', 'अ'], ['व्', 'अ'], ['ऋ'], ['ह्', 'अ'], ['द्', 'अ'], ['आ', 'र्', 'थ्', 'इ', 'क्', 'अ'], ['प्', 'अ'], ['र्', 'अ'], ['ब्', 'अं', 'ध्', 'अ'], ['न्', 'अ'], ['न्', 'अ'], ['इ'], ['ष्', 'अ'], [], ['प्', 'अ'], ['क्', 'ष्', 'अ'], ['त्', 'आ'], ['क्', 'ए'], ['र्', 'ऊ', 'प्', 'अ'], ['म्', 'ए', 'अं'], ['न्', 'अ'], ['य्', 'आ'], ['य्', 'अ'], ['क्', 'ओ'], ['ब्', 'अ'], ['ढ्', 'अ'], ['आ'], ['ए'], ['द्', 'अ'], ['इ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'अ'], [], ['प्', 'अ'], ['ह्', 'ऐ'], ['क्', 'इ'], ['ड्', 'ई'], ['स्', 'ई'], ['ए'], ['च्', 'अ'], ['ए'], ['ल्', 'अ'], ['क्', 'ए'], ['च्', 'अ'], ['ए', 'य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'ए'], ['अं', 'क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['र्', 'अ'], ['ए', 'ड्', 'अ'], ['ड्', 'अ'], ['ई'], ['औ', 'र्', 'अ'], ['व्', 'आ'], ['इ'], ['स्', 'अ'], ['च्', 'अ'], ['ए', 'य्', 'अ'], ['र्', 'अ'], ['म्', 'अ'], ['ऐ', 'न्', 'अ'], ['ट्', 'ई'], ['व्', 'इ'], ['न्', 'आ'], ['य्', 'अ', 'क्', 'अ'], ['र्', 'अ'], ['व्', 'इ'], ['इ', 'स्', 'अ'], ['ब्', 'अ'], ['ऐ'], ['ठ्', 'अ'], ['क्', 'अ'], ['म्', 'ए', 'अं'], ['म्', 'अ'], ['औ'], ['ज्', 'ऊ', 'द्', 'अ'], ['न्', 'अ', 'ह्', 'ई', 'अं'], ['थ्', 'ए']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 143, 'आ': 42, 'क्': 29, 'ए': 25, 'र्': 24, 'न्': 19, 'इ': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ई': 16, 'अं': 15, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'ओ': 9, 'द्': 7, 'ब्': 7}\n",
      "generatedSyllables\n",
      "[['म'], ['ओ'], ['दी'], ['स', 'र', 'का', 'र'], ['के'], ['प', 'ह', 'ले'], ['का', 'र्य'], ['का', 'ल'], ['में'], ['भी'], ['ती', 'न'], ['त'], ['ला'], ['क'], ['को'], ['ले', 'क', 'र'], ['ब'], ['इ', 'ल'], ['ला'], ['या'], ['ग', 'या'], ['था'], ['हा', 'लां', 'कि'], ['त', 'ब'], ['य', 'ह'], ['रा', 'ज्य'], ['स'], ['भा'], ['में'], ['पा', 'स'], ['न', 'हीं'], ['हो'], ['पा', 'या'], ['था'], ['भ'], ['आ', 'ज'], ['पा'], ['के'], ['द'], ['इ', 'व'], ['अं', 'ग'], ['त'], ['ने'], ['ता'], ['प'], ['र', 'म'], ['ओ'], ['द'], ['म'], ['हा'], ['ज', 'न'], ['की'], ['ब'], ['ए', 'ट'], ['ई'], ['पू'], ['न'], ['म'], ['म'], ['हा'], ['ज', 'न'], ['को'], ['स'], ['च'], ['इ', 'व'], ['ब', 'ना', 'या'], ['ग', 'या'], ['है'], ['ऐ', 'सी'], ['स्थि', 'ति'], ['में'], ['ए', 'क'], ['न'], ['या'], ['य'], ['पू', 'र्ण'], ['स', 'र', 'का', 'र'], ['सा', 'र्व', 'ज', 'नि', 'क'], ['वि'], ['त'], ['त'], ['का'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स्ते', 'मा', 'ल'], ['क', 'र', 'ती'], ['है'], ['कि'], ['सं'], ['सा'], ['ध', 'न'], ['ओं'], ['का'], ['आ'], ['व'], ['अं', 'ट'], ['न'], ['स', 'भी'], ['के'], ['उ'], ['प'], ['भ'], ['ओ', 'ग'], ['वा', 'ले'], ['उ'], ['त'], ['पा'], ['द'], ['ओं'], ['की'], ['व'], ['य'], ['व'], ['हा', 'र'], ['य'], ['ता'], ['औ', 'र'], ['स'], ['म'], ['ग'], ['र'], ['व'], ['ऋ'], ['ह'], ['द'], ['आ', 'र्थि', 'क'], ['प'], ['र'], ['बं', 'ध'], ['न'], ['न'], ['इ'], ['ष'], ['प'], ['क्ष'], ['ता'], ['के'], ['रू', 'प'], ['में'], ['न'], ['या'], ['य'], ['को'], ['ब'], ['ढ'], ['आ'], ['ए'], ['द'], ['इ', 'ल'], ['च'], ['स'], ['प'], ['है'], ['कि'], ['डी'], ['सी'], ['ए'], ['च'], ['ए'], ['ल'], ['के'], ['च'], ['ए', 'य'], ['र'], ['म'], ['ऐ', 'न'], ['टी'], ['वे'], ['अं', 'क'], ['ट', 'र'], ['म', 'न'], ['र'], ['ए', 'ड'], ['ड'], ['ई'], ['औ', 'र'], ['वा'], ['इ'], ['स'], ['च'], ['ए', 'य'], ['र'], ['म'], ['ऐ', 'न'], ['टी'], ['वि'], ['ना'], ['य', 'क'], ['र'], ['वि'], ['इ', 'स'], ['ब'], ['ऐ'], ['ठ'], ['क'], ['में'], ['म'], ['औ'], ['जू', 'द'], ['न', 'हीं'], ['थे']]\n",
      "sortedSyllableCount_desc1\n",
      "{'र': 18, 'न': 15, 'स': 11, 'म': 10, 'क': 9, 'इ': 9, 'य': 8, 'ए': 8, 'प': 7, 'त': 7, 'या': 7, 'का': 6, 'ब': 6, 'द': 6, 'व': 6, 'के': 5, 'ल': 5, 'में': 5, 'ग': 5, 'च': 5}\n",
      "bi_characterCount1\n",
      "{'र्अ': 18, 'न्अ': 15, 'स्अ': 11, 'म्अ': 10, 'य्अ': 10, 'क्अ': 9, 'प्अ': 7, 'त्अ': 7, 'य्आ': 7, 'व्अ': 7, 'अर्': 6, 'क्आ': 6, 'आर्': 6, 'ब्अ': 6, 'द्अ': 6, 'अन्': 6, 'क्ए': 5, 'अह्': 5, 'ल्अ': 5, 'म्ए': 5}\n",
      "bi_syllableCount1\n",
      "{'सर': 2, 'रका': 2, 'कार': 2, 'कर': 2, 'इल': 2, 'गया': 2, 'नहीं': 2, 'इव': 2, 'जन': 2, 'इस': 2, 'और': 2, 'एय': 2, 'ऐन': 2, 'पह': 1, 'हले': 1, 'कार्य': 1, 'काल': 1, 'तीन': 1, 'लेक': 1, 'हालां': 1}\n"
     ]
    }
   ],
   "source": [
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndicBERT - vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobufNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/413.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/413.4 kB 262.6 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 112.6/413.4 kB 726.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-4.25.3\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁मद', '▁सर', 'कर', '▁क', '▁पहल', '▁कर', 'य', 'कल', '▁म', '▁भ', '▁तन', '▁तल', 'क', '▁क', '▁', 'लकर', '▁बल', '▁लय', '▁ग', 'य', '▁थ', ',', '▁हलक', '▁तब', '▁यह', '▁रज', 'यस', 'भ', '▁म', '▁पस', '▁न', 'ह', '▁ह', '▁प', 'य', '▁थ', '.', '▁भ', 'ज', 'प', '▁क', '▁', 'दव', 'गत', '▁नत', '▁पर', 'मद', '▁महज', 'न', '▁क', '▁बट', '▁पन', 'म', '▁महज', 'न', '▁क', '▁सच', 'व', '▁बन', 'य', '▁ग', 'य', '▁ह', '.', '▁ऐस', '▁स', 'थ', 'त', '▁म', '▁एक', '▁न', 'य', 'य', 'पर', 'ण', '▁सर', 'कर', '▁सरव', 'जनक', '▁व', 'त', 'त', '▁क', '▁इस', '▁तरह', '▁इस', 'त', 'मल', '▁करत', '▁ह', '▁क', '▁सस', 'धन', '▁क', '▁आव', 'टन', ',', '▁सभ', '▁क', '▁उप', 'भ', 'ग', '▁', 'वल', '▁उत', 'पद', '▁क', '▁वय', 'वह', 'रयत', '▁और', '▁सम', 'गर', '▁वह', 'द', '-', 'आर', 'थक', '▁परब', 'धन', \"▁'\", 'न', 'ष', 'पक', 'ष', 'त', '▁क', '▁', 'रप', '▁म', '▁न', 'य', 'य', \"'\", '▁क', '▁बढ', 'ए', '।', '▁दल', 'च', 'स', 'प', '▁ह', '▁क', '▁', 'डस', 'एचए', 'ल', '▁क', '▁च', 'यर', 'मन', '▁ट', '▁व', 'क', 'टर', 'मन', '▁रड', 'ड', '▁और', '▁व', 'इस', '▁च', 'यर', 'मन', '▁ट', '▁वन', 'यक', '▁', 'रव', '▁इस', '▁ब', 'ठक', '▁म', '▁', 'मजद', '▁न', 'ह', '▁थ', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "IndicBERTtokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "\n",
    "sentence = content\n",
    "# Tokenize the Hindi text\n",
    "tokens = IndicBERTtokenizer.tokenize(IndicBERTtokenizer.decode(IndicBERTtokenizer.encode(sentence, max_length=1000, truncation=True)))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for IndicBERT, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मद', 'सर', 'कर', 'क', 'पहल', 'कर', 'य', 'कल', 'म', 'भ', 'तन', 'तल', 'क', 'क', 'लकर', 'बल', 'लय', 'ग', 'य', 'थ', 'हलक', 'तब', 'यह', 'रज', 'यस', 'भ', 'म', 'पस', 'न', 'ह', 'ह', 'प', 'य', 'थ', 'भ', 'ज', 'प', 'क', 'दव', 'गत', 'नत', 'पर', 'मद', 'महज', 'न', 'क', 'बट', 'पन', 'म', 'महज', 'न', 'क', 'सच', 'व', 'बन', 'य', 'ग', 'य', 'ह', 'ऐस', 'स', 'थ', 'त', 'म', 'एक', 'न', 'य', 'य', 'पर', 'ण', 'सर', 'कर', 'सरव', 'जनक', 'व', 'त', 'त', 'क', 'इस', 'तरह', 'इस', 'त', 'मल', 'करत', 'ह', 'क', 'सस', 'धन', 'क', 'आव', 'टन', 'सभ', 'क', 'उप', 'भ', 'ग', 'वल', 'उत', 'पद', 'क', 'वय', 'वह', 'रयत', 'और', 'सम', 'गर', 'वह', 'द', 'आर', 'थक', 'परब', 'धन', 'न', 'ष', 'पक', 'ष', 'त', 'क', 'रप', 'म', 'न', 'य', 'य', 'क', 'बढ', 'ए', 'दल', 'च', 'स', 'प', 'ह', 'क', 'डस', 'एचए', 'ल', 'क', 'च', 'यर', 'मन', 'ट', 'व', 'क', 'टर', 'मन', 'रड', 'ड', 'और', 'व', 'इस', 'च', 'यर', 'मन', 'ट', 'वन', 'यक', 'रव', 'इस', 'ब', 'ठक', 'म', 'मजद', 'न', 'ह', 'थ']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unicode correction of IndicBERT, vocab size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'अ', 'द्', 'अ'], ['स्', 'अ', 'र्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['क्', 'अ'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['य्', 'अ'], ['क्', 'अ', 'ल्', 'अ'], ['म्', 'अ'], ['भ्', 'अ'], ['त्', 'अ', 'न्', 'अ'], ['त्', 'अ', 'ल्', 'अ'], ['क्', 'अ'], ['क्', 'अ'], ['ल्', 'अ', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'अ', 'ल्', 'अ'], ['ल्', 'अ', 'य्', 'अ'], ['ग्', 'अ'], ['य्', 'अ'], ['थ्', 'अ'], ['ह्', 'अ', 'ल्', 'अ', 'क्', 'अ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'अ', 'ज्', 'अ'], ['य्', 'अ', 'स्', 'अ'], ['भ्', 'अ'], ['म्', 'अ'], ['प्', 'अ', 'स्', 'अ'], ['न्', 'अ'], ['ह्', 'अ'], ['ह्', 'अ'], ['प्', 'अ'], ['य्', 'अ'], ['थ्', 'अ'], ['भ्', 'अ'], ['ज्', 'अ'], ['प्', 'अ'], ['क्', 'अ'], ['द्', 'अ', 'व्', 'अ'], ['ग्', 'अ', 'त्', 'अ'], ['न्', 'अ', 'त्', 'अ'], ['प्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'द्', 'अ'], ['म्', 'अ', 'ह्', 'अ', 'ज्', 'अ'], ['न्', 'अ'], ['क्', 'अ'], ['ब्', 'अ', 'ट्', 'अ'], ['प्', 'अ', 'न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'अ', 'ज्', 'अ'], ['न्', 'अ'], ['क्', 'अ'], ['स्', 'अ', 'च्', 'अ'], ['व्', 'अ'], ['ब्', 'अ', 'न्', 'अ'], ['य्', 'अ'], ['ग्', 'अ'], ['य्', 'अ'], ['ह्', 'अ'], ['ऐ', 'स्', 'अ'], ['स्', 'अ'], ['थ्', 'अ'], ['त्', 'अ'], ['म्', 'अ'], ['ए', 'क्', 'अ'], ['न्', 'अ'], ['य्', 'अ'], ['य्', 'अ'], ['प्', 'अ', 'र्', 'अ'], ['ण्', 'अ'], ['स्', 'अ', 'र्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'व्', 'अ'], ['ज्', 'अ', 'न्', 'अ', 'क्', 'अ'], ['व्', 'अ'], ['त्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'अ'], ['त्', 'अ'], ['म्', 'अ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'त्', 'अ'], ['ह्', 'अ'], ['क्', 'अ'], ['स्', 'अ', 'स्', 'अ'], ['ध्', 'अ', 'न्', 'अ'], ['क्', 'अ'], ['आ', 'व्', 'अ'], ['ट्', 'अ', 'न्', 'अ'], ['स्', 'अ', 'भ्', 'अ'], ['क्', 'अ'], ['उ', 'प्', 'अ'], ['भ्', 'अ'], ['ग्', 'अ'], ['व्', 'अ', 'ल्', 'अ'], ['उ', 'त्', 'अ'], ['प्', 'अ', 'द्', 'अ'], ['क्', 'अ'], ['व्', 'अ', 'य्', 'अ'], ['व्', 'अ', 'ह्', 'अ'], ['र्', 'अ', 'य्', 'अ', 'त्', 'अ'], ['औ', 'र्', 'अ'], ['स्', 'अ', 'म्', 'अ'], ['ग्', 'अ', 'र्', 'अ'], ['व्', 'अ', 'ह्', 'अ'], ['द्', 'अ'], ['आ', 'र्', 'अ'], ['थ्', 'अ', 'क्', 'अ'], ['प्', 'अ', 'र्', 'अ', 'ब्', 'अ'], ['ध्', 'अ', 'न्', 'अ'], ['न्', 'अ'], ['ष्', 'अ'], ['प्', 'अ', 'क्', 'अ'], ['ष्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['र्', 'अ', 'प्', 'अ'], ['म्', 'अ'], ['न्', 'अ'], ['य्', 'अ'], ['य्', 'अ'], ['क्', 'अ'], ['ब्', 'अ', 'ढ्', 'अ'], ['ए'], ['द्', 'अ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'अ'], ['प्', 'अ'], ['ह्', 'अ'], ['क्', 'अ'], ['ड्', 'अ', 'स्', 'अ'], ['ए', 'च्', 'ए'], ['ल्', 'अ'], ['क्', 'अ'], ['च्', 'अ'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['ट्', 'अ'], ['व्', 'अ'], ['क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['र्', 'अ', 'ड्', 'अ'], ['ड्', 'अ'], ['औ', 'र्', 'अ'], ['व्', 'अ'], ['इ', 'स्', 'अ'], ['च्', 'अ'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['ट्', 'अ'], ['व्', 'अ', 'न्', 'अ'], ['य्', 'अ', 'क्', 'अ'], ['र्', 'अ', 'व्', 'अ'], ['इ', 'स्', 'अ'], ['ब्', 'अ'], ['ठ्', 'अ', 'क्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ज्', 'अ', 'द्', 'अ'], ['न्', 'अ'], ['ह्', 'अ'], ['थ्', 'अ']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 242, 'क्': 29, 'र्': 24, 'न्': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'द्': 7, 'ब्': 7, 'ज्': 6, 'भ्': 5, 'ग्': 5, 'थ्': 5, 'ट्': 5, 'च्': 5}\n",
      "generatedSyllables\n",
      "[['म', 'द'], ['स', 'र'], ['क', 'र'], ['क'], ['प', 'ह', 'ल'], ['क', 'र'], ['य'], ['क', 'ल'], ['म'], ['भ'], ['त', 'न'], ['त', 'ल'], ['क'], ['क'], ['ल', 'क', 'र'], ['ब', 'ल'], ['ल', 'य'], ['ग'], ['य'], ['थ'], ['ह', 'ल', 'क'], ['त', 'ब'], ['य', 'ह'], ['र', 'ज'], ['य', 'स'], ['भ'], ['म'], ['प', 'स'], ['न'], ['ह'], ['ह'], ['प'], ['य'], ['थ'], ['भ'], ['ज'], ['प'], ['क'], ['द', 'व'], ['ग', 'त'], ['न', 'त'], ['प', 'र'], ['म', 'द'], ['म', 'ह', 'ज'], ['न'], ['क'], ['ब', 'ट'], ['प', 'न'], ['म'], ['म', 'ह', 'ज'], ['न'], ['क'], ['स', 'च'], ['व'], ['ब', 'न'], ['य'], ['ग'], ['य'], ['ह'], ['ऐ', 'स'], ['स'], ['थ'], ['त'], ['म'], ['ए', 'क'], ['न'], ['य'], ['य'], ['प', 'र'], ['ण'], ['स', 'र'], ['क', 'र'], ['स', 'र', 'व'], ['ज', 'न', 'क'], ['व'], ['त'], ['त'], ['क'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स'], ['त'], ['म', 'ल'], ['क', 'र', 'त'], ['ह'], ['क'], ['स', 'स'], ['ध', 'न'], ['क'], ['आ', 'व'], ['ट', 'न'], ['स', 'भ'], ['क'], ['उ', 'प'], ['भ'], ['ग'], ['व', 'ल'], ['उ', 'त'], ['प', 'द'], ['क'], ['व', 'य'], ['व', 'ह'], ['र', 'य', 'त'], ['औ', 'र'], ['स', 'म'], ['ग', 'र'], ['व', 'ह'], ['द'], ['आ', 'र'], ['थ', 'क'], ['प', 'र', 'ब'], ['ध', 'न'], ['न'], ['ष'], ['प', 'क'], ['ष'], ['त'], ['क'], ['र', 'प'], ['म'], ['न'], ['य'], ['य'], ['क'], ['ब', 'ढ'], ['ए'], ['द', 'ल'], ['च'], ['स'], ['प'], ['ह'], ['क'], ['ड', 'स'], ['ए', 'चे'], ['ल'], ['क'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व'], ['क'], ['ट', 'र'], ['म', 'न'], ['र', 'ड'], ['ड'], ['औ', 'र'], ['व'], ['इ', 'स'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व', 'न'], ['य', 'क'], ['र', 'व'], ['इ', 'स'], ['ब'], ['ठ', 'क'], ['म'], ['म', 'ज', 'द'], ['न'], ['ह'], ['थ']]\n",
      "sortedSyllableCount_desc1\n",
      "{'क': 29, 'र': 24, 'न': 19, 'स': 18, 'य': 17, 'म': 16, 'ह': 14, 'त': 14, 'प': 13, 'व': 13, 'ल': 11, 'द': 7, 'ब': 7, 'ज': 6, 'भ': 5, 'ग': 5, 'थ': 5, 'ट': 5, 'च': 4, 'इ': 4}\n",
      "bi_characterCount1\n",
      "{'क्अ': 29, 'र्अ': 24, 'न्अ': 19, 'स्अ': 18, 'य्अ': 17, 'म्अ': 16, 'अर्': 16, 'ह्अ': 14, 'त्अ': 14, 'प्अ': 13, 'व्अ': 13, 'ल्अ': 11, 'अन्': 11, 'अल्': 8, 'द्अ': 7, 'अह्': 7, 'अक्': 7, 'ब्अ': 7, 'ज्अ': 6, 'भ्अ': 5}\n",
      "bi_syllableCount1\n",
      "{'कर': 5, 'इस': 4, 'सर': 3, 'पर': 3, 'मन': 3, 'मद': 2, 'हल': 2, 'लक': 2, 'मह': 2, 'हज': 2, 'रव': 2, 'धन': 2, 'वह': 2, 'और': 2, 'यर': 2, 'पह': 1, 'कल': 1, 'तन': 1, 'तल': 1, 'बल': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndicBERT - vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁मद', '▁सर', 'कर', '▁क', '▁पहल', '▁कर', 'य', 'कल', '▁म', '▁भ', '▁तन', '▁तल', 'क', '▁क', '▁', 'लकर', '▁बल', '▁लय', '▁ग', 'य', '▁थ', ',', '▁हलक', '▁तब', '▁यह', '▁रज', 'यस', 'भ', '▁म', '▁पस', '▁न', 'ह', '▁ह', '▁प', 'य', '▁थ', '.', '▁भ', 'ज', 'प', '▁क', '▁', 'दव', 'गत', '▁नत', '▁पर', 'मद', '▁महज', 'न', '▁क', '▁बट', '▁पन', 'म', '▁महज', 'न', '▁क', '▁सच', 'व', '▁बन', 'य', '▁ग', 'य', '▁ह', '.', '▁ऐस', '▁स', 'थ', 'त', '▁म', '▁एक', '▁न', 'य', 'य', 'पर', 'ण', '▁सर', 'कर', '▁सरव', 'जनक', '▁व', 'त', 'त', '▁क', '▁इस', '▁तरह', '▁इस', 'त', 'मल', '▁करत', '▁ह', '▁क', '▁सस', 'धन', '▁क', '▁आव', 'टन', ',', '▁सभ', '▁क', '▁उप', 'भ', 'ग', '▁', 'वल', '▁उत', 'पद', '▁क', '▁वय', 'वह', 'रयत', '▁और', '▁सम', 'गर', '▁वह', 'द', '-', 'आर', 'थक', '▁परब', 'धन', \"▁'\", 'न', 'ष', 'पक', 'ष', 'त', '▁क', '▁', 'रप', '▁म', '▁न', 'य', 'य', \"'\", '▁क', '▁बढ', 'ए', '।', '▁दल', 'च', 'स', 'प', '▁ह', '▁क', '▁', 'डस', 'एचए', 'ल', '▁क', '▁च', 'यर', 'मन', '▁ट', '▁व', 'क', 'टर', 'मन', '▁रड', 'ड', '▁और', '▁व', 'इस', '▁च', 'यर', 'मन', '▁ट', '▁वन', 'यक', '▁', 'रव', '▁इस', '▁ब', 'ठक', '▁म', '▁', 'मजद', '▁न', 'ह', '▁थ', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "IndicBERTtokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "\n",
    "sentence = content\n",
    "# Tokenize the Hindi text\n",
    "tokens = IndicBERTtokenizer.tokenize(IndicBERTtokenizer.decode(IndicBERTtokenizer.encode(sentence, max_length=2000, truncation=True)))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tokens for IndicBERT, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मद', 'सर', 'कर', 'क', 'पहल', 'कर', 'य', 'कल', 'म', 'भ', 'तन', 'तल', 'क', 'क', 'लकर', 'बल', 'लय', 'ग', 'य', 'थ', 'हलक', 'तब', 'यह', 'रज', 'यस', 'भ', 'म', 'पस', 'न', 'ह', 'ह', 'प', 'य', 'थ', 'भ', 'ज', 'प', 'क', 'दव', 'गत', 'नत', 'पर', 'मद', 'महज', 'न', 'क', 'बट', 'पन', 'म', 'महज', 'न', 'क', 'सच', 'व', 'बन', 'य', 'ग', 'य', 'ह', 'ऐस', 'स', 'थ', 'त', 'म', 'एक', 'न', 'य', 'य', 'पर', 'ण', 'सर', 'कर', 'सरव', 'जनक', 'व', 'त', 'त', 'क', 'इस', 'तरह', 'इस', 'त', 'मल', 'करत', 'ह', 'क', 'सस', 'धन', 'क', 'आव', 'टन', 'सभ', 'क', 'उप', 'भ', 'ग', 'वल', 'उत', 'पद', 'क', 'वय', 'वह', 'रयत', 'और', 'सम', 'गर', 'वह', 'द', 'आर', 'थक', 'परब', 'धन', 'न', 'ष', 'पक', 'ष', 'त', 'क', 'रप', 'म', 'न', 'य', 'य', 'क', 'बढ', 'ए', 'दल', 'च', 'स', 'प', 'ह', 'क', 'डस', 'एचए', 'ल', 'क', 'च', 'यर', 'मन', 'ट', 'व', 'क', 'टर', 'मन', 'रड', 'ड', 'और', 'व', 'इस', 'च', 'यर', 'मन', 'ट', 'वन', 'यक', 'रव', 'इस', 'ब', 'ठक', 'म', 'मजद', 'न', 'ह', 'थ']\n"
     ]
    }
   ],
   "source": [
    "processedUnigram=[]\n",
    "for word in tokens:\n",
    "  str=\"\"\n",
    "  for ch in word:\n",
    "    if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "      str+=ch\n",
    "  if len(str)>0:\n",
    "    processedUnigram.append(str)\n",
    "\n",
    "\n",
    "print(processedUnigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of IndicBERT, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁ह', 'नद', '▁भ', 'ष', '▁क', '▁अद', 'वत', 'य', '▁स', 'थन', '▁ह', '।', '▁आज', '▁हम', '▁आप', 'क', '▁ल', 'ए', '▁लय', '▁ह', '▁जद', 'ग', '▁पर', '▁श', 'यर', '▁द', '▁ल', 'इन', '▁जनक', '▁आप', '▁अपन', '▁दस', 'त', '▁क', '▁स', 'थ', '▁स', 'झ', '▁कर', '▁सक', 'त', '▁ह', '।', '▁अपन', '▁दस', 'त', '▁क', '▁स', 'थ', '▁स', 'झ', '▁कर', '▁सक', 'त', '▁ह', '।', '▁सव', 'रग', '▁आट', '▁च', 'द', '[SEP]']\n",
      "['ह', 'नद', 'भ', 'ष', 'क', 'अद', 'वत', 'य', 'स', 'थन', 'ह', 'आज', 'हम', 'आप', 'क', 'ल', 'ए', 'लय', 'ह', 'जद', 'ग', 'पर', 'श', 'यर', 'द', 'ल', 'इन', 'जनक', 'आप', 'अपन', 'दस', 'त', 'क', 'स', 'थ', 'स', 'झ', 'कर', 'सक', 'त', 'ह', 'अपन', 'दस', 'त', 'क', 'स', 'थ', 'स', 'झ', 'कर', 'सक', 'त', 'ह', 'सव', 'रग', 'आट', 'च', 'द']\n",
      "[['ह्', 'अ'], ['न्', 'अ', 'द्', 'अ'], ['भ्', 'अ'], ['ष्', 'अ'], ['क्', 'अ'], ['अ', 'द्', 'अ'], ['व्', 'अ', 'त्', 'अ'], ['य्', 'अ'], ['स्', 'अ'], ['थ्', 'अ', 'न्', 'अ'], ['ह्', 'अ'], ['आ', 'ज्', 'अ'], ['ह्', 'अ', 'म्', 'अ'], ['आ', 'प्', 'अ'], ['क्', 'अ'], ['ल्', 'अ'], ['ए'], ['ल्', 'अ', 'य्', 'अ'], ['ह्', 'अ'], ['ज्', 'अ', 'द्', 'अ'], ['ग्', 'अ'], ['प्', 'अ', 'र्', 'अ'], ['श्', 'अ'], ['य्', 'अ', 'र्', 'अ'], ['द्', 'अ'], ['ल्', 'अ'], ['इ', 'न्', 'अ'], ['ज्', 'अ', 'न्', 'अ', 'क्', 'अ'], ['आ', 'प्', 'अ'], ['अ', 'प्', 'अ', 'न्', 'अ'], ['द्', 'अ', 'स्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['स्', 'अ'], ['थ्', 'अ'], ['स्', 'अ'], ['झ्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'क्', 'अ'], ['त्', 'अ'], ['ह्', 'अ'], ['अ', 'प्', 'अ', 'न्', 'अ'], ['द्', 'अ', 'स्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['स्', 'अ'], ['थ्', 'अ'], ['स्', 'अ'], ['झ्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'क्', 'अ'], ['त्', 'अ'], ['ह्', 'अ'], ['स्', 'अ', 'व्', 'अ'], ['र्', 'अ', 'ग्', 'अ'], ['आ', 'ट्', 'अ'], ['च्', 'अ'], ['द्', 'अ']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 242, 'क्': 29, 'र्': 24, 'न्': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'द्': 7, 'ब्': 7, 'ज्': 6, 'भ्': 5, 'ग्': 5, 'थ्': 5, 'ट्': 5, 'च्': 5}\n",
      "generatedSyllables\n",
      "[['म', 'द'], ['स', 'र'], ['क', 'र'], ['क'], ['प', 'ह', 'ल'], ['क', 'र'], ['य'], ['क', 'ल'], ['म'], ['भ'], ['त', 'न'], ['त', 'ल'], ['क'], ['क'], ['ल', 'क', 'र'], ['ब', 'ल'], ['ल', 'य'], ['ग'], ['य'], ['थ'], ['ह', 'ल', 'क'], ['त', 'ब'], ['य', 'ह'], ['र', 'ज'], ['य', 'स'], ['भ'], ['म'], ['प', 'स'], ['न'], ['ह'], ['ह'], ['प'], ['य'], ['थ'], ['भ'], ['ज'], ['प'], ['क'], ['द', 'व'], ['ग', 'त'], ['न', 'त'], ['प', 'र'], ['म', 'द'], ['म', 'ह', 'ज'], ['न'], ['क'], ['ब', 'ट'], ['प', 'न'], ['म'], ['म', 'ह', 'ज'], ['न'], ['क'], ['स', 'च'], ['व'], ['ब', 'न'], ['य'], ['ग'], ['य'], ['ह'], ['ऐ', 'स'], ['स'], ['थ'], ['त'], ['म'], ['ए', 'क'], ['न'], ['य'], ['य'], ['प', 'र'], ['ण'], ['स', 'र'], ['क', 'र'], ['स', 'र', 'व'], ['ज', 'न', 'क'], ['व'], ['त'], ['त'], ['क'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स'], ['त'], ['म', 'ल'], ['क', 'र', 'त'], ['ह'], ['क'], ['स', 'स'], ['ध', 'न'], ['क'], ['आ', 'व'], ['ट', 'न'], ['स', 'भ'], ['क'], ['उ', 'प'], ['भ'], ['ग'], ['व', 'ल'], ['उ', 'त'], ['प', 'द'], ['क'], ['व', 'य'], ['व', 'ह'], ['र', 'य', 'त'], ['औ', 'र'], ['स', 'म'], ['ग', 'र'], ['व', 'ह'], ['द'], ['आ', 'र'], ['थ', 'क'], ['प', 'र', 'ब'], ['ध', 'न'], ['न'], ['ष'], ['प', 'क'], ['ष'], ['त'], ['क'], ['र', 'प'], ['म'], ['न'], ['य'], ['य'], ['क'], ['ब', 'ढ'], ['ए'], ['द', 'ल'], ['च'], ['स'], ['प'], ['ह'], ['क'], ['ड', 'स'], ['ए', 'चे'], ['ल'], ['क'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व'], ['क'], ['ट', 'र'], ['म', 'न'], ['र', 'ड'], ['ड'], ['औ', 'र'], ['व'], ['इ', 'स'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व', 'न'], ['य', 'क'], ['र', 'व'], ['इ', 'स'], ['ब'], ['ठ', 'क'], ['म'], ['म', 'ज', 'द'], ['न'], ['ह'], ['थ']]\n",
      "sortedSyllableCount_desc1\n",
      "{'क': 29, 'र': 24, 'न': 19, 'स': 18, 'य': 17, 'म': 16, 'ह': 14, 'त': 14, 'प': 13, 'व': 13, 'ल': 11, 'द': 7, 'ब': 7, 'ज': 6, 'भ': 5, 'ग': 5, 'थ': 5, 'ट': 5, 'च': 4, 'इ': 4}\n",
      "bi_characterCount1\n",
      "{'क्अ': 29, 'र्अ': 24, 'न्अ': 19, 'स्अ': 18, 'य्अ': 17, 'म्अ': 16, 'अर्': 16, 'ह्अ': 14, 'त्अ': 14, 'प्अ': 13, 'व्अ': 13, 'ल्अ': 11, 'अन्': 11, 'अल्': 8, 'द्अ': 7, 'अह्': 7, 'अक्': 7, 'ब्अ': 7, 'ज्अ': 6, 'भ्अ': 5}\n",
      "bi_syllableCount1\n",
      "{'कर': 5, 'इस': 4, 'सर': 3, 'पर': 3, 'मन': 3, 'मद': 2, 'हल': 2, 'लक': 2, 'मह': 2, 'हज': 2, 'रव': 2, 'धन': 2, 'वह': 2, 'और': 2, 'यर': 2, 'पह': 1, 'कल': 1, 'तन': 1, 'तल': 1, 'बल': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHITESPACE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मोदी', 'सरकार', 'के', 'पहले', 'कार्यकाल', 'में', 'भी', 'तीन', 'तलाक', 'को', 'लेकर', 'बिल', 'लाया', 'गया', 'था,', 'हालांकि', 'तब', 'यह', 'राज्यसभा', 'में', 'पास', 'नहीं', 'हो', 'पाया', 'था.', 'भाजपा', 'के', 'दिवंगत', 'नेता', 'प्रमोद', 'महाजन', 'की', 'बेटी', 'पूनम', 'महाजन', 'को', 'सचिव', 'बनाया', 'गया', 'है.', 'ऐसी', 'स्थिति', 'में', 'एक', 'न्यायपूर्ण', 'सरकार', 'सार्वजनिक', 'वित्त', 'का', 'इस', 'तरह', 'इस्तेमाल', 'करती', 'है', 'कि', 'संसाधनों', 'का', 'आवंटन,', 'सभी', 'के', 'उपभोग', 'वाले', 'उत्पादों', 'की', 'व्यवहार्यता', 'और', 'समग्र', 'वृहद-आर्थिक', 'प्रबंधन', \"'निष्पक्षता\", 'के', 'रूप', 'में', \"न्याय'\", 'को', 'बढ़ाए।', 'दिलचस्प', 'है', 'कि', 'डीसीएचएल', 'के', 'चेयरमैन', 'टी', 'वेंकटरमन', 'रेड्डी', 'और', 'वाइस', 'चेयरमैन', 'टी', 'विनायक', 'रवि', 'इस', 'बैठक', 'में', 'मौजूद', 'नहीं', 'थे।']\n"
     ]
    }
   ],
   "source": [
    "tokens = content.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode correction of Whitespace tokenizer, vocab size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['म्', 'अ', 'द्', 'अ'], ['स्', 'अ', 'र्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['क्', 'अ'], ['प्', 'अ', 'ह्', 'अ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['य्', 'अ'], ['क्', 'अ', 'ल्', 'अ'], ['म्', 'अ'], ['भ्', 'अ'], ['त्', 'अ', 'न्', 'अ'], ['त्', 'अ', 'ल्', 'अ'], ['क्', 'अ'], ['क्', 'अ'], ['ल्', 'अ', 'क्', 'अ', 'र्', 'अ'], ['ब्', 'अ', 'ल्', 'अ'], ['ल्', 'अ', 'य्', 'अ'], ['ग्', 'अ'], ['य्', 'अ'], ['थ्', 'अ'], ['ह्', 'अ', 'ल्', 'अ', 'क्', 'अ'], ['त्', 'अ', 'ब्', 'अ'], ['य्', 'अ', 'ह्', 'अ'], ['र्', 'अ', 'ज्', 'अ'], ['य्', 'अ', 'स्', 'अ'], ['भ्', 'अ'], ['म्', 'अ'], ['प्', 'अ', 'स्', 'अ'], ['न्', 'अ'], ['ह्', 'अ'], ['ह्', 'अ'], ['प्', 'अ'], ['य्', 'अ'], ['थ्', 'अ'], ['भ्', 'अ'], ['ज्', 'अ'], ['प्', 'अ'], ['क्', 'अ'], ['द्', 'अ', 'व्', 'अ'], ['ग्', 'अ', 'त्', 'अ'], ['न्', 'अ', 'त्', 'अ'], ['प्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'द्', 'अ'], ['म्', 'अ', 'ह्', 'अ', 'ज्', 'अ'], ['न्', 'अ'], ['क्', 'अ'], ['ब्', 'अ', 'ट्', 'अ'], ['प्', 'अ', 'न्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ह्', 'अ', 'ज्', 'अ'], ['न्', 'अ'], ['क्', 'अ'], ['स्', 'अ', 'च्', 'अ'], ['व्', 'अ'], ['ब्', 'अ', 'न्', 'अ'], ['य्', 'अ'], ['ग्', 'अ'], ['य्', 'अ'], ['ह्', 'अ'], ['ऐ', 'स्', 'अ'], ['स्', 'अ'], ['थ्', 'अ'], ['त्', 'अ'], ['म्', 'अ'], ['ए', 'क्', 'अ'], ['न्', 'अ'], ['य्', 'अ'], ['य्', 'अ'], ['प्', 'अ', 'र्', 'अ'], ['ण्', 'अ'], ['स्', 'अ', 'र्', 'अ'], ['क्', 'अ', 'र्', 'अ'], ['स्', 'अ', 'र्', 'अ', 'व्', 'अ'], ['ज्', 'अ', 'न्', 'अ', 'क्', 'अ'], ['व्', 'अ'], ['त्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['इ', 'स्', 'अ'], ['त्', 'अ', 'र्', 'अ', 'ह्', 'अ'], ['इ', 'स्', 'अ'], ['त्', 'अ'], ['म्', 'अ', 'ल्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'त्', 'अ'], ['ह्', 'अ'], ['क्', 'अ'], ['स्', 'अ', 'स्', 'अ'], ['ध्', 'अ', 'न्', 'अ'], ['क्', 'अ'], ['आ', 'व्', 'अ'], ['ट्', 'अ', 'न्', 'अ'], ['स्', 'अ', 'भ्', 'अ'], ['क्', 'अ'], ['उ', 'प्', 'अ'], ['भ्', 'अ'], ['ग्', 'अ'], ['व्', 'अ', 'ल्', 'अ'], ['उ', 'त्', 'अ'], ['प्', 'अ', 'द्', 'अ'], ['क्', 'अ'], ['व्', 'अ', 'य्', 'अ'], ['व्', 'अ', 'ह्', 'अ'], ['र्', 'अ', 'य्', 'अ', 'त्', 'अ'], ['औ', 'र्', 'अ'], ['स्', 'अ', 'म्', 'अ'], ['ग्', 'अ', 'र्', 'अ'], ['व्', 'अ', 'ह्', 'अ'], ['द्', 'अ'], ['आ', 'र्', 'अ'], ['थ्', 'अ', 'क्', 'अ'], ['प्', 'अ', 'र्', 'अ', 'ब्', 'अ'], ['ध्', 'अ', 'न्', 'अ'], ['न्', 'अ'], ['ष्', 'अ'], ['प्', 'अ', 'क्', 'अ'], ['ष्', 'अ'], ['त्', 'अ'], ['क्', 'अ'], ['र्', 'अ', 'प्', 'अ'], ['म्', 'अ'], ['न्', 'अ'], ['य्', 'अ'], ['य्', 'अ'], ['क्', 'अ'], ['ब्', 'अ', 'ढ्', 'अ'], ['ए'], ['द्', 'अ', 'ल्', 'अ'], ['च्', 'अ'], ['स्', 'अ'], ['प्', 'अ'], ['ह्', 'अ'], ['क्', 'अ'], ['ड्', 'अ', 'स्', 'अ'], ['ए', 'च्', 'ए'], ['ल्', 'अ'], ['क्', 'अ'], ['च्', 'अ'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['ट्', 'अ'], ['व्', 'अ'], ['क्', 'अ'], ['ट्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['र्', 'अ', 'ड्', 'अ'], ['ड्', 'अ'], ['औ', 'र्', 'अ'], ['व्', 'अ'], ['इ', 'स्', 'अ'], ['च्', 'अ'], ['य्', 'अ', 'र्', 'अ'], ['म्', 'अ', 'न्', 'अ'], ['ट्', 'अ'], ['व्', 'अ', 'न्', 'अ'], ['य्', 'अ', 'क्', 'अ'], ['र्', 'अ', 'व्', 'अ'], ['इ', 'स्', 'अ'], ['ब्', 'अ'], ['ठ्', 'अ', 'क्', 'अ'], ['म्', 'अ'], ['म्', 'अ', 'ज्', 'अ', 'द्', 'अ'], ['न्', 'अ'], ['ह्', 'अ'], ['थ्', 'अ']]\n"
     ]
    }
   ],
   "source": [
    "unigramCharacters=unigramCharacter(processedUnigram)\n",
    "print(unigramCharacter(processedUnigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 uni-gram and bi-gram frequencies of tokens, syllables, and characters for each of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sortedCharacterCount_desc1\n",
      "{'अ': 242, 'क्': 29, 'र्': 24, 'न्': 19, 'स्': 18, 'य्': 17, 'म्': 16, 'ह्': 14, 'त्': 14, 'प्': 13, 'व्': 13, 'ल्': 11, 'द्': 7, 'ब्': 7, 'ज्': 6, 'भ्': 5, 'ग्': 5, 'थ्': 5, 'ट्': 5, 'च्': 5}\n",
      "generatedSyllables\n",
      "[['म', 'द'], ['स', 'र'], ['क', 'र'], ['क'], ['प', 'ह', 'ल'], ['क', 'र'], ['य'], ['क', 'ल'], ['म'], ['भ'], ['त', 'न'], ['त', 'ल'], ['क'], ['क'], ['ल', 'क', 'र'], ['ब', 'ल'], ['ल', 'य'], ['ग'], ['य'], ['थ'], ['ह', 'ल', 'क'], ['त', 'ब'], ['य', 'ह'], ['र', 'ज'], ['य', 'स'], ['भ'], ['म'], ['प', 'स'], ['न'], ['ह'], ['ह'], ['प'], ['य'], ['थ'], ['भ'], ['ज'], ['प'], ['क'], ['द', 'व'], ['ग', 'त'], ['न', 'त'], ['प', 'र'], ['म', 'द'], ['म', 'ह', 'ज'], ['न'], ['क'], ['ब', 'ट'], ['प', 'न'], ['म'], ['म', 'ह', 'ज'], ['न'], ['क'], ['स', 'च'], ['व'], ['ब', 'न'], ['य'], ['ग'], ['य'], ['ह'], ['ऐ', 'स'], ['स'], ['थ'], ['त'], ['म'], ['ए', 'क'], ['न'], ['य'], ['य'], ['प', 'र'], ['ण'], ['स', 'र'], ['क', 'र'], ['स', 'र', 'व'], ['ज', 'न', 'क'], ['व'], ['त'], ['त'], ['क'], ['इ', 'स'], ['त', 'र', 'ह'], ['इ', 'स'], ['त'], ['म', 'ल'], ['क', 'र', 'त'], ['ह'], ['क'], ['स', 'स'], ['ध', 'न'], ['क'], ['आ', 'व'], ['ट', 'न'], ['स', 'भ'], ['क'], ['उ', 'प'], ['भ'], ['ग'], ['व', 'ल'], ['उ', 'त'], ['प', 'द'], ['क'], ['व', 'य'], ['व', 'ह'], ['र', 'य', 'त'], ['औ', 'र'], ['स', 'म'], ['ग', 'र'], ['व', 'ह'], ['द'], ['आ', 'र'], ['थ', 'क'], ['प', 'र', 'ब'], ['ध', 'न'], ['न'], ['ष'], ['प', 'क'], ['ष'], ['त'], ['क'], ['र', 'प'], ['म'], ['न'], ['य'], ['य'], ['क'], ['ब', 'ढ'], ['ए'], ['द', 'ल'], ['च'], ['स'], ['प'], ['ह'], ['क'], ['ड', 'स'], ['ए', 'चे'], ['ल'], ['क'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व'], ['क'], ['ट', 'र'], ['म', 'न'], ['र', 'ड'], ['ड'], ['औ', 'र'], ['व'], ['इ', 'स'], ['च'], ['य', 'र'], ['म', 'न'], ['ट'], ['व', 'न'], ['य', 'क'], ['र', 'व'], ['इ', 'स'], ['ब'], ['ठ', 'क'], ['म'], ['म', 'ज', 'द'], ['न'], ['ह'], ['थ']]\n",
      "sortedSyllableCount_desc1\n",
      "{'क': 29, 'र': 24, 'न': 19, 'स': 18, 'य': 17, 'म': 16, 'ह': 14, 'त': 14, 'प': 13, 'व': 13, 'ल': 11, 'द': 7, 'ब': 7, 'ज': 6, 'भ': 5, 'ग': 5, 'थ': 5, 'ट': 5, 'च': 4, 'इ': 4}\n",
      "bi_characterCount1\n",
      "{'क्अ': 29, 'र्अ': 24, 'न्अ': 19, 'स्अ': 18, 'य्अ': 17, 'म्अ': 16, 'अर्': 16, 'ह्अ': 14, 'त्अ': 14, 'प्अ': 13, 'व्अ': 13, 'ल्अ': 11, 'अन्': 11, 'अल्': 8, 'द्अ': 7, 'अह्': 7, 'अक्': 7, 'ब्अ': 7, 'ज्अ': 6, 'भ्अ': 5}\n",
      "bi_syllableCount1\n",
      "{'कर': 5, 'इस': 4, 'सर': 3, 'पर': 3, 'मन': 3, 'मद': 2, 'हल': 2, 'लक': 2, 'मह': 2, 'हज': 2, 'रव': 2, 'धन': 2, 'वह': 2, 'और': 2, 'यर': 2, 'पह': 1, 'कल': 1, 'तन': 1, 'तल': 1, 'बल': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sortedCharacterCount_desc1 = sortedCharacterCount_desc(unigramCharacters)\n",
    "generatedSyllables = syllable_generator(unigramCharacters)\n",
    "sortedSyllableCount_desc1 = sortedSyllableCount_desc(generatedSyllables)\n",
    "bi_characterCount1 = bi_characterCount(unigramCharacters)\n",
    "bi_syllableCount1 = bi_SyllableCount(generatedSyllables)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sortedCharacterCount_desc1\")\n",
    "# print(sortedCharacterCount_desc1)\n",
    "top_20_sortedCharacterCount_desc = dict(list(sortedCharacterCount_desc1.items())[:20])\n",
    "print(top_20_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "print(\"generatedSyllables\")\n",
    "print(generatedSyllables)\n",
    "\n",
    "\n",
    "print(\"sortedSyllableCount_desc1\")\n",
    "# print(sortedSyllableCount_desc1)\n",
    "top_20_sortedSyllableCount_desc = dict(list(sortedSyllableCount_desc1.items())[:20])\n",
    "print(top_20_sortedSyllableCount_desc)\n",
    "\n",
    "\n",
    "print(\"bi_characterCount1\")\n",
    "# print(bi_characterCount1)\n",
    "top_20_bi_sortedCharacterCount_desc = dict(list(bi_characterCount1.items())[:20])\n",
    "print(top_20_bi_sortedCharacterCount_desc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bi_syllableCount1\")\n",
    "# print(bi_syllableCount1)\n",
    "top_20_bi_sortedSyllableCount_desc = dict(list(bi_syllableCount1.items())[:20])\n",
    "print(top_20_bi_sortedSyllableCount_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " def calulateprf(ground_truth_set,sample_token_set):\n",
    "    # Convert both sets to sets for efficient intersection and difference operations\n",
    "    \n",
    "    \n",
    "    TP,FP,FN=0.0,0.0,0.0\n",
    "    \n",
    "    for i in range(len(ground_truth_set)):\n",
    "\n",
    "        # Calculate True Positives (TP)\n",
    "        TP += len(set(sample_token_set[i]).intersection(set(ground_truth_set[i])))\n",
    "\n",
    "        # Calculate False Positives (FP)\n",
    "        FP += len(set(sample_token_set[i]).difference(set(ground_truth_set[i])))\n",
    "\n",
    "        # Calculate False Negatives (FN)\n",
    "        FN += len(set(ground_truth_set[i]).difference(set(sample_token_set[i])))\n",
    "\n",
    "    # Calculate Precision\n",
    "    precision = TP / (TP + FP) if (TP + FP)>0 else 0.0\n",
    "\n",
    "    # Calculate Recall\n",
    "    recall = TP / (TP + FN) if (TP + FP)>0 else 0.0\n",
    "\n",
    "    # Calculate F-score\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall)>0 else 0.0\n",
    "\n",
    "    return precision,recall,f_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cs689_assignment.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read and print the content\n",
    "#     print(file.read())\n",
    "    content=file.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list=['जहां पर पूर्व विधानसभा अध्यक्ष डॉ. राधा रमण शास्त्री के साथ विधायक बलवीर वर्मा शामिल हैं', 'इन हजारों परिवारों के सदस्यों ने जहां जोरदार धरने प्रदर्शन किए, वहीं हाईकोर्ट में याचिका दायर की', 'इससे पहले भी कई अफेयर हुए हैं जिनमें कुछ तो सफल रिलेशनशिप में तब्दील हो गए वहीं कुछ अफेयर सिर्फ अफसाना बनकर रह गए।', 'रिपोर्ट की माने तो\\xa0इस पर एक भारतीय\\xa0अधिकारी ने\\xa0कहा सरकार विदेशी मोटरसाइकिलों पर आयात शुल्क अधिक लगाती है क्योंकि घरेलू उद्योग को सुरक्षा देने के लिए यह जरूरी है', '2- अब इसे 20 मिनट के लिए ढक कर रख दें. अब एक पैन में चीनी और पानी डालकर 5 मिनट तक उबालें. जब चीनी पूरी तरह से mix हो जाए तब इसमें इलायची पाउडर डालकर अच्छे से मिलाएं और ढक कर रख दें', ': वनडेकीएकपारीमेंसबसेज्यादाचौकेवछक्के', 'इस बीच उन्होंने गेंद के साथ भी जरुरत पड़ने पर अपना योगदान दिया', 'रमन सिंह पर फूल बरसाने को नहीं मिल रहे आमजन, शिक्षकों को जारी हुआ सीएम पर फूल बरसाने का लिखित आदे', '11 लोगों ने विपक्ष की उम्मीदवार मीरा कुमार की बजाय रामनाथ कोविंद के लिए मतदान किया था', 'मीडिया रिपोर्टों के मुताबिक एक चर्च के बाहर सुरक्षाकर्मी एक महिला और उसके दो बच्चों को राेक कर उनसे पूछताछ कर रही थी', 'आज की तारीख है 10 अगस्त 2018 तथा आज इस वर्ष का 222वां दिन है। इन 222 दिनों में प्रारंग अपने परिवार के जौनपुरवासियों तक 208 लेख पहुंचा चुका है तथा यह लेख 209वां लेख होगा।', 'गौरतलब है कि शेयर बाजारों में मंगलवार को लगातार दूसरे दिन गिरावट आयी और सेंसेक्स 642 अंक टूटकर 36,481.09 अंक पर बंद हुआ', 'गौरतलब है कि स्वास्थ्य की दिशा में स्वस्थ भारत पिछले 3 वर्षों से लगातार काम कर रहा है। पिछले वर्ष स्वस्थ बालिका स्वस्थ समाज क', 'सुमन मल्लाैर की, सुखबीर बने बड़ौली के सरपं', 'दूसरा पहलू है- क़ानून-व्यवस्था के संभालने में राज्य और स्थानीय प्रशासन की विफलता', 'वे न तो हनफ़ी, हंबली, शाफ़ेई और मालेकी को मानते हैं और न ही शिया मुसलमानों को।', 'कि वह भगवान या भगवान जैसा लगने लगे', 'इस मौके पर छात्र नेताओं ने कहा कि छात्रों के हित के लिए हर स्तर पर प्रयास किया जाएगा। अगर प्राचार्य हमारी मांगों को नहीं मानते तो आगे भी आंदोलन किया जाएगा। बाद में मौके पर पहुंचे प्राचार्य ने छात्रों को आश्वासन दिया', 'इससे गरीब एवं पीड़ितों को सहायता मिलेगी', 'लीड परीक्षा से पहले कॉलेज ने ऑनलाइन मोड के माध्यम से आधिकारिक साइट पर बीएओयू टाइम टेबल 201 9(BAOU Time Table 2019) का प्रचार किया है।', 'अत: मेरी प्रार्थना ना सिर्फ मेरे आत्म हित के लिए है अपितु संसार के हर भूले भटके, जीव को ईश्वर और संत योगी व गुरू कृपा प्राप्त हो।सच्चे गुरुजन की सीख, प्रत्येक जीव को सन्मार्ग पर चलते रहने के लिए प्रेरित करे', 'तिब्बती आध्यात्मिक धर्मगुरु दलाई लामा इन दिनों बोधगया में हैं', 'इस विचार के साथ दिक्कत यह है कि केवल 37 फीसद भारतीयों के पास बैंक खाते हैं और भारत में लगभग 80 फीसद लेनदेन नकद में होता है', 'इतनी दुआ कर दो हमारे लिए कि जितना प्यार दुनिया ने आपको दिया है, बस उतना ही हमें भी मिल जाए|', 'सरकार चलाते थ']\n",
    "ans_list=[['जहां पर', ' पूर्व विधानसभा अध्यक्ष', ' डॉ. राधा रमण शास्त्री', ' के साथ', ' विधायक बलवीर वर्मा', ' शामिल हैं'], ['इन हजारों परिवारों के सदस्यों', ' ने जहां जोरदार धरने प्रदर्शन किए', ' वहीं हाईकोर्ट में', ' याचिका दायर की'], ['इससे पहले भी', ' कई अफेयर हुए हैं', ' जिनमें', ' कुछ तो', ' सफल रिलेशनशिप में', ' तब्दील हो गए', ' वहीं', ' कुछ अफेयर', ' सिर्फ अफसाना बनकर रह गए'], [' रिपोर्ट की माने तो', ' इस पर', ' एक भारतीय अधिकारी', ' ने कहा', ' सरकार विदेशी मोटरसाइकिलों पर', ' आयात शुल्क अधिक लगाती है', ' क्योंकि घरेलू उद्योग को सुरक्षा देने के लिए यह जरूरी है'], ['2- अब इसे', ' 20 मिनट के लिए', ' ढक कर रख दें.', ' अब एक पैन में', ' चीनी और पानी डालकर', ' 5 मिनट तक उबालें.', ' जब चीनी पूरी तरह से', ' mix हो जाए', ' तब इसमें', ' इलायची पाउडर डालकर', ' अच्छे से मिलाएं', ' और ढक कर रख दें'], ['वनडे की एक', ' पारी में', ' सबसे ज्यादा', ' चौके व छक्क'], ['इस बीच', ' उन्होंने गेंद के साथ भी', ' जरुरत पड़ने पर', ' अपना योगदान दिया'], ['रमन सिंह पर फूल बरसाने को नहीं मिल रहे आमजन', ' शिक्षकों को जारी हुआ', ' सीएम पर फूल बरसाने का लिखित आदे'], ['11 लोगों ने', ' विपक्ष की उम्मीदवार मीरा कुमार की बजाय', ' रामनाथ कोविंद के लिए', ' मतदान किया था'], ['मीडिया रिपोर्टों के मुताबिक', ' एक चर्च के बाहर', ' सुरक्षाकर्मी', ' एक महिला और उसके दो बच्चों को', ' राेक कर', ' उनसे पूछताछ कर रही थी'], [' आज की तारीख है 10 अगस्त 2018', ' तथा आज इस वर्ष का 222वां दिन है।', ' इन 222 दिनों में', ' प्रारंग अपने परिवार के जौनपुरवासियों तक 208 लेख पहुंचा चुका है', ' तथा यह लेख 209वां लेख होगा'], ['गौरतलब है कि शेयर बाजारों में', ' मंगलवार को', ' लगातार दूसरे दिन', ' गिरावट आयी', ' और सेंसेक्स', ' 642 अंक', ' टूटकर', ' 36', '481.09 अंक पर', ' बंद हुआ'], ['गौरतलब है कि', ' स्वास्थ्य की दिशा में', ' स्वस्थ भारत', ' पिछले 3 वर्षों से', ' लगातार काम कर रहा है।', ' पिछले वर्ष', ' स्वस्थ बालिका', ' स्वस्थ समाज क'], ['सुमन मल्लाैर की', ' सुखबीर बने बड़ौली के सरपं'], ['दूसरा पहलू है', '- क़ानून-व्यवस्था के संभालने में', ' राज्य और स्थानीय प्रशासन की विफलता'], ['वे न तो हनफ़ी', ' हंबली', ' शाफ़ेई और मालेकी को मानते हैं', ' और न ही शिया मुसलमानों को'], ['कि वह', ' भगवान या भगवान जैसा लगने लगे'], ['इस मौके पर', ' छात्र नेताओं ने कहा', ' कि छात्रों के हित के लिए', ' हर स्तर पर प्रयास किया जाएगा।', ' अगर प्राचार्य', ' हमारी मांगों को नहीं मानते', ' तो आगे भी', ' आंदोलन किया जाएगा।', ' बाद में मौके पर पहुंचे प्राचार्य ने', ' छात्रों को आश्वासन दिया'], ['इससे', ' गरीब एवं पीड़ितों को', ' सहायता मिलेगी'], [' लीड परीक्षा से पहले', ' कॉलेज ने', ' ऑनलाइन मोड के माध्यम से', ' आधिकारिक साइट पर', ' बीएओयू टाइम टेबल 201 9(BAOU Time Table 2019) का प्रचार किया है'], ['अत: मेरी प्रार्थना', ' ना सिर्फ मेरे आत्म हित के लिए है', ' अपितु संसार के हर भूले भटके', ' जीव को ईश्वर और संत योगी व गुरू कृपा प्राप्त हो।', 'सच्चे गुरुजन की सीख', ' प्रत्येक जीव को', ' सन्मार्ग पर चलते रहने के लिए प्रेरित करे'], ['तिब्बती', ' आध्यात्मिक', ' धर्मगुरु', ' दलाई लामा', ' इन दिनों', ' बोधगया', ' में हैं'], ['\"इस विचार के साथ दिक्कत यह है', ' कि केवल 37 फीसद भारतीयों के पास बैंक खाते हैं', ' और भारत में लगभग 80 फीसद लेनदेन नकद में होता है।'], ['\"इतनी दुआ कर दो', ' हमारे लिए', ' कि जितना प्यार दुनिया ने आपको दिया है', ' बस उतना ही हमें भी मिल जाए।'], ['सरकार', ' चलाते थ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025974025974025974 0.014925373134328358 0.004424778761061947\n"
     ]
    }
   ],
   "source": [
    "unigram_token_list=[]\n",
    "for i in ques_list:\n",
    "    unigram_token_list.append(sp.encode_as_pieces(i))\n",
    "\n",
    "sample_token_set=[]\n",
    "for tokens in unigram_token_list:\n",
    "    l=[]\n",
    "    for word in tokens:\n",
    "      str=\"\"\n",
    "      for ch in word:\n",
    "        if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "          str+=ch\n",
    "      l.append(str)\n",
    "    sample_token_set.append(l)\n",
    "    \n",
    "p,r,f=calulateprf(ans_list,sample_token_set)\n",
    "print(p,r,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_token_list=[]\n",
    "for i in ques_list:\n",
    "    bpe_token_list.append(bpe.encode_as_pieces(i))\n",
    "\n",
    "sample_token_set=[]\n",
    "for tokens in bpe_token_list:\n",
    "    l=[]\n",
    "    for word in tokens:\n",
    "      str=\"\"\n",
    "      for ch in word:\n",
    "        if ch in matra or ch  in vowels or ch in consonants or ch=='्':\n",
    "          str+=ch\n",
    "      l.append(str)\n",
    "    sample_token_set.append(l)\n",
    "    \n",
    "p,r,f=calulateprf(ans_list,sample_token_set)\n",
    "print(p,r,f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
